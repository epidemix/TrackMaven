{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('https://s3.amazonaws.com/temp-data-pulls/newdump.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "rawpin_blog = df[(df[\"type\"]==\"pin\") | (df[\"type\"]==\"blog post\")]\n",
    "rawpin_blog.drop([\"has_spend\"], axis = 1, inplace=True)\n",
    "channel_info = rawpin_blog['channel_info'].apply(pd.Series)\n",
    "channel_info.columns = [\"channel\", \"info\"]\n",
    "content_info = rawpin_blog['content'].apply(pd.Series)\n",
    "content_info.drop(['author_email', 'content', 'pinned_from'], axis=1, inplace=True) ## THESE HAVE ONLY NULLS\n",
    "for x in content_info.columns:\n",
    "    if \"count\" in x:\n",
    "        content_info[x].fillna(np.NaN, inplace = True)\n",
    "        #content_info[x] = content_info[x].astype(int)\n",
    "master_pinblog = rawpin_blog.join(channel_info).join(content_info)\n",
    "master_pinblog.drop(['channel_info', 'content'], axis = 1, inplace = True)\n",
    "master_pinblog.columns = ['brand', 'engagement', 'uniqueid', 'impact', 'share_token', 'timestamp',\n",
    "       'type', 'urls', 'channel', 'info', 'author_name', 'comment_count',\n",
    "       'description', 'fb_likecount', 'fb_sharecount',\n",
    "       'gplus_count', 'hashtags', 'image_url', 'like_count',\n",
    "       'link', 'linkedin_sharecount', 'links', 'pin_id', 'pin_url',\n",
    "       'pin_count', 'post_type', 'repin_count', 'summary',\n",
    "       'thumbnail_url', 'title', 'tweet_count']\n",
    "\n",
    "master_pinblog[\"links_count\"] = master_pinblog['links'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = master_pinblog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new.link = df_new.link.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_medium_df(df,medium):\n",
    "    # create new df called blogs that only contains blogs\n",
    "    df = df[df['type'] == medium]\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    if medium == 'blog post':\n",
    "        print('Creating {} DataFrame...'.format(medium))\n",
    "        # converts link to string so we can split\n",
    "        df['link'] = df['link'].astype(str)\n",
    "        # instantiate a new list called new_mag\n",
    "        new_mag = []\n",
    "        # list comprehension that just keeps part before '.com'\n",
    "        # we can use list comprehension because this is true for all values\n",
    "        magazine = [i.split('.com')[0] for i in df['link']]\n",
    "        # start for loop to get rid of everything before the name of the magazine\n",
    "        for i in magazine:\n",
    "            if '.' in i:\n",
    "                new_mag.append(i.split('.')[1])\n",
    "        # if there isn't a '.' it just sends the existing name to the list\n",
    "            else:\n",
    "                new_mag.append(i)\n",
    "        # create new column for the blog df with the publications\n",
    "        df['pub'] = new_mag\n",
    "    else:\n",
    "        print('Creating {} DataFrame...'.format(medium))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating blog post DataFrame...\n"
     ]
    }
   ],
   "source": [
    "medium = create_medium_df(df_new,'blog post')\n",
    "pub = publication_df(medium, 'glamour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11097, 34)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def publication_df(df, publication):\n",
    "    pub_df = df[df['pub'] == publication]\n",
    "    return pub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(df, series, stop_words = True):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    en_stop = get_stop_words('en')\n",
    "    # when a word is lemmatized, contractions are rightfully turned into different stems since 's = is\n",
    "    # however, in reality, all of those words are themselves stop words, so I want to exclude them\n",
    "    # question marks and the like are not helpful for our purpose of figuring out potential categories\n",
    "    contractions = [\"'s\",\"s\",\"'\",\".\",\",\",\"n't\",\"'d\",\"ll\",\"re\",\"ve\",\"``\",\n",
    "                    \"''\",\"”\",\"“\",\"’\",\"(\",\")\",\"?\",\":\",\"t\",\";\",\"d\",\"!\",\"-\",\"[\",\"]\",\"w\",\"#\",\"m\"]\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    post_text = [i for i in df[series]]\n",
    "    count = 1\n",
    "    print(f\"Initializing tokenizer and lemmatizer ...\")\n",
    "    print(\"Number of posts tokenized and lemmatized:\")\n",
    "    for i in post_text:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "        \n",
    "        if stop_words == True:\n",
    "            # stem tokens and remove stop words\n",
    "            lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens if not i in en_stop]\n",
    "        else:\n",
    "            lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "        \n",
    "        #remove stemmed contractions\n",
    "        contracted_tokens = [i for i in lemmed_tokens if not i in contractions]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(contracted_tokens)\n",
    "        if count % 5000 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    print(\"Lemmatizing Completed.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(list_):\n",
    "    print('Removing stop words...')\n",
    "    en_stop = get_stop_words('en')\n",
    "    no_stop_words = [i for i in list_ if not i in en_stop]\n",
    "    print('Stop Word Removal Complete.')\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_feature_extraction(df, series, lem_list, word_list):\n",
    "    print('Initializing title feature extraction...')\n",
    "    print('Initializing word count for title length...')\n",
    "    # need to tokenize and lemmatize to count the length\n",
    "    stopped_titles = remove_stop_words(lem_list)\n",
    "    # adding to dataframe\n",
    "    df['title_length'] = [len(stopped_titles[i]) for i in range(len(stopped_titles))]\n",
    "    celeb = ['beyonce','kim','karsashian','taylor','swift','justin','bieber','rihanna','scarlet','johansson','dwayne','johnson',\n",
    "    'ellen','degeneres','katy','perry','angelina','jolie','drake','brad','pitt','jay','cristiano','ronaldo','jennifer',\n",
    "    'aniston','oprah','winfrey','adele','jonny','depp','tom','cruise','jennifer','lopez','sean','colms','jennifer','lawrence',\n",
    "    'leonardo','dicaprio','sandra','bullock','selena','gomez','tom','hanks','julia','roberts','howard','stern','donald',\n",
    "    'trump','robert','downey','britney','spears','adam','sandler','megan','fox','kylie','jenner','miley','cyrus','jessica',\n",
    "    'alba','emma','watson','eminem','paris','hilton','vin','diesel','kevin','hart','will','smith','chris','rock',\n",
    "    'chris','hemsworth','chris','pratt','ben','affleck','matt','damon','denzel','washington']\n",
    "\n",
    "    print('Extracting other attributes from titles...')\n",
    "    # the following code is a bunch of different feature extractions for the titles\n",
    "    df['title_is_question'] = ['?' in i for i in df[series]]\n",
    "    df['title_contains_number'] = [any(x in i for x in ['1','2','3','4','5','6','7','8','9','0']) for i in df[series]]\n",
    "    df['title_contains_celeb'] = [any(x in i for x in celeb) for i in df[series]]\n",
    "    for j in word_list:\n",
    "        print(f'Creating column for {j}')\n",
    "        df[\"title_contains_{}\".format(j)] = [j in i.lower() for i in df[series]] \n",
    "    print('Title Feature Extraction Complete')\n",
    "    full_df = df\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify different groupings of words which count as topics\n",
    "def topic_modeling(df, lem_list,number_of_topics = 5,number_of_words = 30,number_of_passes = 3):\n",
    "    print(\"Initializing Topic Modeling...\")\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(lem_list)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in lem_list]\n",
    "    # generate LDA model\n",
    "    print(\"Generating Model...\")\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=number_of_topics, id2word = dictionary, passes=number_of_passes)\n",
    "    topics = ldamodel.print_topics(num_topics=number_of_topics, num_words=number_of_words)\n",
    "    print(\"Topics\\n\")\n",
    "    for i in range(number_of_topics):\n",
    "        print(f\"Topic {topics[i][0]}: \\n\")\n",
    "        print(topics[i][1], \"\\n\")\n",
    "    #return ldamodel[corpus]\n",
    "    topic_vector = ldamodel[corpus]\n",
    "    #return topic_vector\n",
    "    print(\"Adding topic probabilities to DataFrame...\")\n",
    "    for j in range(number_of_topics):\n",
    "        print(f'Adding Topic {j}...')\n",
    "        df[\"Topic_{}\".format(j)] = [topic_vector[i][j][1] if len(topic_vector[i]) == number_of_topics else np.NaN for i in range(len(topic_vector))]\n",
    "    print(\"Percetange Of Observations Missing Topic Values:\")\n",
    "    print(df['Topic_0'].isnull().sum()/df.shape[0]*100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def early_pipeline(df_entry, medium_type, publication, list_of_words):\n",
    "    df = create_medium_df(df_entry, medium_type)\n",
    "    word_list = [i.lower() for i in list_of_words]\n",
    "    lemmatized_titles = None\n",
    "    try:\n",
    "        if medium_type == 'blog post':\n",
    "            df = publication_df(df, publication)\n",
    "            lemmatized_titles = lemmatizing(df, 'title', stop_words = False)\n",
    "            new_titles = lemmatizing(df, 'title', stop_words = True)\n",
    "            df = title_feature_extraction(df, 'title', lemmatized_titles, word_list)\n",
    "        elif medium_type == 'pin':\n",
    "            lemmatized_titles = lemmatizing(df, 'description', stop_words = False)\n",
    "            df = title_feature_extraction(df, 'description', lemmatized_titles, word_list)\n",
    "            new_titles = lemmatizing(df, 'description', stop_words = True)\n",
    "        df = topic_modeling(df, lem_list = new_titles)\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Invalid entries.\\n Use either 'blog post' or 'pin'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating blog post DataFrame...\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n",
      "Initializing title feature extraction...\n",
      "Initializing word count for title length...\n",
      "Removing stop words...\n",
      "Stop Word Removal Complete.\n",
      "Extracting other attributes from titles...\n",
      "Creating column for best\n",
      "Creating column for sex\n",
      "Creating column for now\n",
      "Creating column for new\n",
      "Creating column for episode\n",
      "Creating column for how\n",
      "Title Feature Extraction Complete\n",
      "Initializing Topic Modeling...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.021*\"best\" + 0.017*\"2016\" + 0.013*\"hair\" + 0.009*\"beauty\" + 0.009*\"idea\" + 0.009*\"makeup\" + 0.008*\"outfit\" + 0.008*\"2017\" + 0.008*\"new\" + 0.007*\"hadid\" + 0.006*\"celebrity\" + 0.006*\"red\" + 0.006*\"look\" + 0.006*\"trend\" + 0.006*\"can\" + 0.005*\"wear\" + 0.005*\"product\" + 0.005*\"will\" + 0.004*\"day\" + 0.004*\"gift\" + 0.004*\"gigi\" + 0.004*\"woman\" + 0.004*\"carpet\" + 0.004*\"show\" + 0.004*\"color\" + 0.004*\"blake\" + 0.004*\"fall\" + 0.004*\"get\" + 0.004*\"skin\" + 0.004*\"love\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.018*\"woman\" + 0.009*\"taylor\" + 0.009*\"new\" + 0.008*\"swift\" + 0.007*\"just\" + 0.005*\"thing\" + 0.005*\"obama\" + 0.005*\"sex\" + 0.005*\"one\" + 0.005*\"first\" + 0.004*\"chrissy\" + 0.004*\"teigen\" + 0.004*\"open\" + 0.004*\"season\" + 0.004*\"girl\" + 0.004*\"year\" + 0.004*\"get\" + 0.004*\"watch\" + 0.004*\"life\" + 0.004*\"video\" + 0.004*\"story\" + 0.003*\"big\" + 0.003*\"food\" + 0.003*\"real\" + 0.003*\"make\" + 0.003*\"say\" + 0.003*\"talk\" + 0.003*\"will\" + 0.003*\"much\" + 0.003*\"men\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.015*\"kardashian\" + 0.014*\"new\" + 0.010*\"kim\" + 0.007*\"get\" + 0.007*\"like\" + 0.005*\"star\" + 0.005*\"will\" + 0.004*\"look\" + 0.004*\"instagram\" + 0.004*\"trailer\" + 0.004*\"first\" + 0.004*\"throne\" + 0.004*\"say\" + 0.004*\"watch\" + 0.004*\"movie\" + 0.004*\"sex\" + 0.004*\"model\" + 0.003*\"video\" + 0.003*\"detail\" + 0.003*\"west\" + 0.003*\"secret\" + 0.003*\"make\" + 0.003*\"skin\" + 0.003*\"life\" + 0.003*\"work\" + 0.003*\"body\" + 0.003*\"job\" + 0.003*\"disney\" + 0.003*\"game\" + 0.003*\"emma\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.012*\"jenner\" + 0.011*\"summer\" + 0.010*\"new\" + 0.009*\"just\" + 0.008*\"season\" + 0.007*\"show\" + 0.007*\"kylie\" + 0.006*\"selena\" + 0.006*\"gomez\" + 0.006*\"like\" + 0.006*\"kendall\" + 0.005*\"got\" + 0.005*\"woman\" + 0.005*\"right\" + 0.005*\"need\" + 0.005*\"tattoo\" + 0.004*\"know\" + 0.004*\"get\" + 0.004*\"girl\" + 0.004*\"wear\" + 0.004*\"bachelorette\" + 0.004*\"now\" + 0.004*\"picture\" + 0.004*\"recap\" + 0.004*\"can\" + 0.004*\"look\" + 0.004*\"fashion\" + 0.004*\"5\" + 0.004*\"tatum\" + 0.004*\"7\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.017*\"trump\" + 0.011*\"donald\" + 0.011*\"2016\" + 0.009*\"dress\" + 0.009*\"wedding\" + 0.009*\"woman\" + 0.009*\"clinton\" + 0.008*\"summer\" + 0.008*\"style\" + 0.008*\"best\" + 0.008*\"hillary\" + 0.007*\"fashion\" + 0.005*\"people\" + 0.005*\"met\" + 0.005*\"gala\" + 0.005*\"photo\" + 0.004*\"day\" + 0.004*\"$\" + 0.004*\"harry\" + 0.004*\"according\" + 0.004*\"jennifer\" + 0.004*\"mother\" + 0.004*\"perry\" + 0.004*\"worst\" + 0.004*\"katy\" + 0.003*\"victoria\" + 0.003*\"look\" + 0.003*\"secret\" + 0.003*\"every\" + 0.003*\"white\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Adding Topic 0...\n",
      "Adding Topic 1...\n",
      "Adding Topic 2...\n",
      "Adding Topic 3...\n",
      "Adding Topic 4...\n",
      "Percetange Of Observations Missing Topic Values:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "words = ['best','sex','now','new','episode','how']\n",
    "df = early(df_new, medium_type = 'blog post', publication = 'glamour', list_of_words = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_percent = df.sort_values(by='impact', ascending = False).iloc[:round(df.shape[0]/4)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
