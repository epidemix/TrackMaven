{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('https://s3.amazonaws.com/temp-data-pulls/newdump.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "rawpin_blog = df[(df[\"type\"]==\"pin\") | (df[\"type\"]==\"blog post\")]\n",
    "rawpin_blog.drop([\"has_spend\"], axis = 1, inplace=True)\n",
    "channel_info = rawpin_blog['channel_info'].apply(pd.Series)\n",
    "channel_info.columns = [\"channel\", \"info\"]\n",
    "content_info = rawpin_blog['content'].apply(pd.Series)\n",
    "content_info.drop(['author_email', 'content', 'pinned_from'], axis=1, inplace=True) ## THESE HAVE ONLY NULLS\n",
    "for x in content_info.columns:\n",
    "    if \"count\" in x:\n",
    "        content_info[x].fillna(np.NaN, inplace = True)\n",
    "        #content_info[x] = content_info[x].astype(int)\n",
    "master_pinblog = rawpin_blog.join(channel_info).join(content_info)\n",
    "master_pinblog.drop(['channel_info', 'content'], axis = 1, inplace = True)\n",
    "master_pinblog.columns = ['brand', 'engagement', 'uniqueid', 'impact', 'share_token', 'timestamp',\n",
    "       'type', 'urls', 'channel', 'info', 'author_name', 'comment_count',\n",
    "       'description', 'fb_likecount', 'fb_sharecount',\n",
    "       'gplus_count', 'hashtags', 'image_url', 'like_count',\n",
    "       'link', 'linkedin_sharecount', 'links', 'pin_id', 'pin_url',\n",
    "       'pin_count', 'post_type', 'repin_count', 'summary',\n",
    "       'thumbnail_url', 'title', 'tweet_count']\n",
    "\n",
    "master_pinblog[\"links_count\"] = master_pinblog['links'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = master_pinblog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new.link = df_new.link.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_medium_df(df,medium):\n",
    "    # create new df called blogs that only contains blogs\n",
    "    df = df[df['type'] == medium]\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    if medium == 'blog post':\n",
    "        print('Creating {} DataFrame...'.format(medium))\n",
    "        # converts link to string so we can split\n",
    "        df['link'] = df['link'].astype(str)\n",
    "        # instantiate a new list called new_mag\n",
    "        new_mag = []\n",
    "        # list comprehension that just keeps part before '.com'\n",
    "        # we can use list comprehension because this is true for all values\n",
    "        magazine = [i.split('.com')[0] for i in df['link']]\n",
    "        # start for loop to get rid of everything before the name of the magazine\n",
    "        for i in magazine:\n",
    "            if '.' in i:\n",
    "                new_mag.append(i.split('.')[1])\n",
    "        # if there isn't a '.' it just sends the existing name to the list\n",
    "            else:\n",
    "                new_mag.append(i)\n",
    "        # create new column for the blog df with the publications\n",
    "        df['pub'] = new_mag\n",
    "    else:\n",
    "        print('Creating {} DataFrame...'.format(medium))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def publication_df(df, publication):\n",
    "    pub_df = df[df['pub'] == publication]\n",
    "    return pub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(df, series, stop_words = True):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    en_stop = get_stop_words('en')\n",
    "    # when a word is lemmatized, contractions are rightfully turned into different stems since 's = is\n",
    "    # however, in reality, all of those words are themselves stop words, so I want to exclude them\n",
    "    # question marks and the like are not helpful for our purpose of figuring out potential categories\n",
    "    contractions = [\"'s\",\"s\",\"'\",\".\",\",\",\"n't\",\"'d\",\"ll\",\"re\",\"ve\",\"``\",\n",
    "                    \"''\",\"”\",\"“\",\"’\",\"(\",\")\",\"?\",\":\",\"t\",\";\",\"d\",\"!\",\"-\",\"[\",\"]\",\"w\",\"#\",\"m\"]\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    post_text = [i for i in df[series]]\n",
    "    count = 1\n",
    "    print(f\"Initializing tokenizer and lemmatizer ...\")\n",
    "    print(\"Number of posts tokenized and lemmatized:\")\n",
    "    for i in post_text:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "        \n",
    "        if stop_words == True:\n",
    "            # stem tokens and remove stop words\n",
    "            lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens if not i in en_stop]\n",
    "        else:\n",
    "            lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "        \n",
    "        #remove stemmed contractions\n",
    "        contracted_tokens = [i for i in lemmed_tokens if not i in contractions]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(contracted_tokens)\n",
    "        if count % 5000 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    print(\"Lemmatizing Completed.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(list_):\n",
    "    print('Removing stop words...')\n",
    "    en_stop = get_stop_words('en')\n",
    "    no_stop_words = [i for i in list_ if not i in en_stop]\n",
    "    print('Stop Word Removal Complete.')\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_feature_extraction(df, series, lem_list, word_list):\n",
    "    print('Initializing title feature extraction...')\n",
    "    print('Initializing word count for title length...')\n",
    "    # need to tokenize and lemmatize to count the length\n",
    "    stopped_titles = remove_stop_words(lem_list)\n",
    "    # adding to dataframe\n",
    "    df['title_length'] = [len(stopped_titles[i]) for i in range(len(stopped_titles))]\n",
    "    celeb = ['beyonce','kim','karsashian','taylor','swift','justin','bieber','rihanna','scarlet','johansson','dwayne','johnson',\n",
    "    'ellen','degeneres','katy','perry','angelina','jolie','drake','brad','pitt','jay','cristiano','ronaldo','jennifer',\n",
    "    'aniston','oprah','winfrey','adele','jonny','depp','tom','cruise','jennifer','lopez','sean','colms','jennifer','lawrence',\n",
    "    'leonardo','dicaprio','sandra','bullock','selena','gomez','tom','hanks','julia','roberts','howard','stern','donald',\n",
    "    'trump','robert','downey','britney','spears','adam','sandler','megan','fox','kylie','jenner','miley','cyrus','jessica',\n",
    "    'alba','emma','watson','eminem','paris','hilton','vin','diesel','kevin','hart','will','smith','chris','rock',\n",
    "    'chris','hemsworth','chris','pratt','ben','affleck','matt','damon','denzel','washington']\n",
    "\n",
    "    print('Extracting other attributes from titles...')\n",
    "    # the following code is a bunch of different feature extractions for the titles\n",
    "    df['title_is_question'] = ['?' in i for i in df[series]]\n",
    "    df['title_contains_number'] = [any(x in i for x in ['1','2','3','4','5','6','7','8','9','0']) for i in df[series]]\n",
    "    df['title_contains_celeb'] = [any(x in i for x in celeb) for i in df[series]]\n",
    "    for j in word_list:\n",
    "        print(f'Creating column for {j}')\n",
    "        df[\"title_contains_{}\".format(j)] = [j in i.lower() for i in df[series]] \n",
    "    print('Title Feature Extraction Complete')\n",
    "    full_df = df\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify different groupings of words which count as topics\n",
    "def topic_modeling(df, lem_list,number_of_topics = 5,number_of_words = 30,number_of_passes = 3):\n",
    "    print(\"Initializing Topic Modeling...\")\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(lem_list)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in lem_list]\n",
    "    # generate LDA model\n",
    "    print(\"Generating Model...\")\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=number_of_topics, id2word = dictionary, passes=number_of_passes)\n",
    "    topics = ldamodel.print_topics(num_topics=number_of_topics, num_words=number_of_words)\n",
    "    print(\"Topics\\n\")\n",
    "    for i in range(number_of_topics):\n",
    "        print(f\"Topic {topics[i][0]}: \\n\")\n",
    "        print(topics[i][1], \"\\n\")\n",
    "    #return ldamodel[corpus]\n",
    "    topic_vector = ldamodel[corpus]\n",
    "    #return topic_vector\n",
    "    print(\"Adding topic probabilities to DataFrame...\")\n",
    "    for j in range(number_of_topics):\n",
    "        print(f'Adding Topic {j}...')\n",
    "        df[\"Topic_{}\".format(j)] = [topic_vector[i][j][1] if len(topic_vector[i]) == number_of_topics else np.NaN for i in range(len(topic_vector))]\n",
    "    print(\"Percetange Of Observations Missing Topic Values:\")\n",
    "    print(df['Topic_0'].isnull().sum()/df.shape[0]*100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def early_pipeline(df_entry, medium_type, publication, list_of_words):\n",
    "    df = create_medium_df(df_entry, medium_type)\n",
    "    word_list = [i.lower() for i in list_of_words]\n",
    "    lemmatized_titles = None\n",
    "    try:\n",
    "        if medium_type == 'blog post':\n",
    "            df = publication_df(df, publication)\n",
    "            lemmatized_titles = lemmatizing(df, 'title', stop_words = False)\n",
    "            new_titles = lemmatizing(df, 'title', stop_words = True)\n",
    "            df = title_feature_extraction(df, 'title', lemmatized_titles, word_list)\n",
    "        elif medium_type == 'pin':\n",
    "            lemmatized_titles = lemmatizing(df, 'description', stop_words = False)\n",
    "            df = title_feature_extraction(df, 'description', lemmatized_titles, word_list)\n",
    "            new_titles = lemmatizing(df, 'description', stop_words = True)\n",
    "        df = topic_modeling(df, lem_list = new_titles)\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Invalid entries.\\n Use either 'blog post' or 'pin'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating blog post DataFrame...\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n",
      "Initializing title feature extraction...\n",
      "Initializing word count for title length...\n",
      "Removing stop words...\n",
      "Stop Word Removal Complete.\n",
      "Extracting other attributes from titles...\n",
      "Creating column for best\n",
      "Creating column for sex\n",
      "Creating column for now\n",
      "Creating column for new\n",
      "Creating column for episode\n",
      "Creating column for how\n",
      "Title Feature Extraction Complete\n",
      "Initializing Topic Modeling...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.021*\"best\" + 0.017*\"2016\" + 0.013*\"hair\" + 0.009*\"beauty\" + 0.009*\"idea\" + 0.009*\"makeup\" + 0.008*\"outfit\" + 0.008*\"2017\" + 0.008*\"new\" + 0.007*\"hadid\" + 0.006*\"celebrity\" + 0.006*\"red\" + 0.006*\"look\" + 0.006*\"trend\" + 0.006*\"can\" + 0.005*\"wear\" + 0.005*\"product\" + 0.005*\"will\" + 0.004*\"day\" + 0.004*\"gift\" + 0.004*\"gigi\" + 0.004*\"woman\" + 0.004*\"carpet\" + 0.004*\"show\" + 0.004*\"color\" + 0.004*\"blake\" + 0.004*\"fall\" + 0.004*\"get\" + 0.004*\"skin\" + 0.004*\"love\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.018*\"woman\" + 0.009*\"taylor\" + 0.009*\"new\" + 0.008*\"swift\" + 0.007*\"just\" + 0.005*\"thing\" + 0.005*\"obama\" + 0.005*\"sex\" + 0.005*\"one\" + 0.005*\"first\" + 0.004*\"chrissy\" + 0.004*\"teigen\" + 0.004*\"open\" + 0.004*\"season\" + 0.004*\"girl\" + 0.004*\"year\" + 0.004*\"get\" + 0.004*\"watch\" + 0.004*\"life\" + 0.004*\"video\" + 0.004*\"story\" + 0.003*\"big\" + 0.003*\"food\" + 0.003*\"real\" + 0.003*\"make\" + 0.003*\"say\" + 0.003*\"talk\" + 0.003*\"will\" + 0.003*\"much\" + 0.003*\"men\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.015*\"kardashian\" + 0.014*\"new\" + 0.010*\"kim\" + 0.007*\"get\" + 0.007*\"like\" + 0.005*\"star\" + 0.005*\"will\" + 0.004*\"look\" + 0.004*\"instagram\" + 0.004*\"trailer\" + 0.004*\"first\" + 0.004*\"throne\" + 0.004*\"say\" + 0.004*\"watch\" + 0.004*\"movie\" + 0.004*\"sex\" + 0.004*\"model\" + 0.003*\"video\" + 0.003*\"detail\" + 0.003*\"west\" + 0.003*\"secret\" + 0.003*\"make\" + 0.003*\"skin\" + 0.003*\"life\" + 0.003*\"work\" + 0.003*\"body\" + 0.003*\"job\" + 0.003*\"disney\" + 0.003*\"game\" + 0.003*\"emma\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.012*\"jenner\" + 0.011*\"summer\" + 0.010*\"new\" + 0.009*\"just\" + 0.008*\"season\" + 0.007*\"show\" + 0.007*\"kylie\" + 0.006*\"selena\" + 0.006*\"gomez\" + 0.006*\"like\" + 0.006*\"kendall\" + 0.005*\"got\" + 0.005*\"woman\" + 0.005*\"right\" + 0.005*\"need\" + 0.005*\"tattoo\" + 0.004*\"know\" + 0.004*\"get\" + 0.004*\"girl\" + 0.004*\"wear\" + 0.004*\"bachelorette\" + 0.004*\"now\" + 0.004*\"picture\" + 0.004*\"recap\" + 0.004*\"can\" + 0.004*\"look\" + 0.004*\"fashion\" + 0.004*\"5\" + 0.004*\"tatum\" + 0.004*\"7\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.017*\"trump\" + 0.011*\"donald\" + 0.011*\"2016\" + 0.009*\"dress\" + 0.009*\"wedding\" + 0.009*\"woman\" + 0.009*\"clinton\" + 0.008*\"summer\" + 0.008*\"style\" + 0.008*\"best\" + 0.008*\"hillary\" + 0.007*\"fashion\" + 0.005*\"people\" + 0.005*\"met\" + 0.005*\"gala\" + 0.005*\"photo\" + 0.004*\"day\" + 0.004*\"$\" + 0.004*\"harry\" + 0.004*\"according\" + 0.004*\"jennifer\" + 0.004*\"mother\" + 0.004*\"perry\" + 0.004*\"worst\" + 0.004*\"katy\" + 0.003*\"victoria\" + 0.003*\"look\" + 0.003*\"secret\" + 0.003*\"every\" + 0.003*\"white\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Adding Topic 0...\n",
      "Adding Topic 1...\n",
      "Adding Topic 2...\n",
      "Adding Topic 3...\n",
      "Adding Topic 4...\n",
      "Percetange Of Observations Missing Topic Values:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "words = ['best','sex','now','new','episode','how']\n",
    "df = early(df_new, medium_type = 'blog post', publication = 'glamour', list_of_words = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pin DataFrame...\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "Lemmatizing Completed.\n",
      "Initializing title feature extraction...\n",
      "Initializing word count for title length of **         index   brand  engagement                            uniqueid  \\\n",
      "0            1  137299           0  MTM3Mjk5LTEzNzEzMDIxX3Bpbl8xNzQ4Mw   \n",
      "1            5  137299           1  MTM3Mjk5LTEzNzEzMDIyX3Bpbl8xNzQ4Mw   \n",
      "2            7  137299           0  MTM3Mjk5LTEzNzEzMDIzX3Bpbl8xNzQ4Mw   \n",
      "3           11  137299           0  MTM3Mjk5LTEzNzEzMDI0X3Bpbl8xNzQ4Mw   \n",
      "4           19  137326           4  MTM3MzI2LTEzNzEzNjQ3X3Bpbl8xNzQ4Mw   \n",
      "5           21  137299           0  MTM3Mjk5LTEzNzEwMjMyX3Bpbl8xNzQ4Mw   \n",
      "6           24  137299           0  MTM3Mjk5LTEzNzEwMjMzX3Bpbl8xNzQ4Mw   \n",
      "7           29  137299           1  MTM3Mjk5LTEzNzEwMjM0X3Bpbl8xNzQ4Mw   \n",
      "8           31  137299           0  MTM3Mjk5LTEzNzEwMjM1X3Bpbl8xNzQ4Mw   \n",
      "9           34  137299           0  MTM3Mjk5LTEzNzEwMjM2X3Bpbl8xNzQ4Mw   \n",
      "10          35  137326          10  MTM3MzI2LTEzNzEzNjQ4X3Bpbl8xNzQ4Mw   \n",
      "11          43  137299           1  MTM3Mjk5LTEzNzEwMjM3X3Bpbl8xNzQ4Mw   \n",
      "12          46  137299           0  MTM3Mjk5LTEzNzEwMjM4X3Bpbl8xNzQ4Mw   \n",
      "13          50  137299           0  MTM3Mjk5LTEzNzEwMjM5X3Bpbl8xNzQ4Mw   \n",
      "14          52  137299           2  MTM3Mjk5LTEzNzEwMjQwX3Bpbl8xNzQ4Mw   \n",
      "15          57  137299           0  MTM3Mjk5LTEzNzEwMjQxX3Bpbl8xNzQ4Mw   \n",
      "16          65  137326           6  MTM3MzI2LTEzNzEzNjQ5X3Bpbl8xNzQ4Mw   \n",
      "17          68  137299           0  MTM3Mjk5LTEzNzEwMjQyX3Bpbl8xNzQ4Mw   \n",
      "18          69  137299          11  MTM3Mjk5LTEzNzEwMjQzX3Bpbl8xNzQ4Mw   \n",
      "19          73  137299           0  MTM3Mjk5LTEzNzEwMjQ0X3Bpbl8xNzQ4Mw   \n",
      "20          74  137299           0  MTM3Mjk5LTEzNzEwMjQ1X3Bpbl8xNzQ4Mw   \n",
      "21          79  137299           0  MTM3Mjk5LTEzNzEwMjQ2X3Bpbl8xNzQ4Mw   \n",
      "22          83  137299           0  MTM3Mjk5LTEzNzEwMjQ3X3Bpbl8xNzQ4Mw   \n",
      "23          84  137326           8  MTM3MzI2LTEzNzEzNjUwX3Bpbl8xNzQ4Mw   \n",
      "24          91  137299           0  MTM3Mjk5LTEzNzEwMjQ4X3Bpbl8xNzQ4Mw   \n",
      "25          95  137299           0  MTM3Mjk5LTEzNzEwMjQ5X3Bpbl8xNzQ4Mw   \n",
      "26          98  137299           0  MTM3Mjk5LTEzNzEwMjUwX3Bpbl8xNzQ4Mw   \n",
      "27         100  137299           0  MTM3Mjk5LTEzNzEwMjUxX3Bpbl8xNzQ4Mw   \n",
      "28         102  137299           0  MTM3Mjk5LTEzNzEwMjUyX3Bpbl8xNzQ4Mw   \n",
      "29         107  137299           0  MTM3Mjk5LTEzNzEwMjUzX3Bpbl8xNzQ4Mw   \n",
      "...        ...     ...         ...                                 ...   \n",
      "132770  758548  137299           3         MTM3Mjk5LTYyMzI0MTFfcGluXzA   \n",
      "132771  758553  137299           1         MTM3Mjk5LTYyMzI0MTJfcGluXzA   \n",
      "132772  758562  137299           4         MTM3Mjk5LTYyMzI0MTNfcGluXzA   \n",
      "132773  758571  137299           2         MTM3Mjk5LTYyMzI0MTRfcGluXzA   \n",
      "132774  758577  137299          45         MTM3Mjk5LTYyMzI0MTVfcGluXzA   \n",
      "132775  758578  137299          11         MTM3Mjk5LTYyMzI0MTZfcGluXzA   \n",
      "132776  758586  137299           7         MTM3Mjk5LTYyMzI0MTdfcGluXzA   \n",
      "132777  758591  137299           1         MTM3Mjk5LTYyMzI0MThfcGluXzA   \n",
      "132778  758594  137299           5         MTM3Mjk5LTYyMzI0MTlfcGluXzA   \n",
      "132779  758596  137299           5         MTM3Mjk5LTYyMzI0MjBfcGluXzA   \n",
      "132780  758597  137299          23         MTM3Mjk5LTYyMzI0MjFfcGluXzA   \n",
      "132781  758607  137299          26         MTM3Mjk5LTYyMzI0MjJfcGluXzA   \n",
      "132782  758612  137299           3         MTM3Mjk5LTYyMzE2MzhfcGluXzA   \n",
      "132783  758618  137299           4         MTM3Mjk5LTYyMzE2MzlfcGluXzA   \n",
      "132784  758621  137299           2         MTM3Mjk5LTYyMzE2NDBfcGluXzA   \n",
      "132785  758628  137299           4         MTM3Mjk5LTYyMzE2NDFfcGluXzA   \n",
      "132786  758633  137299           1         MTM3Mjk5LTYyMzE2NDJfcGluXzA   \n",
      "132787  758641  137299           0         MTM3Mjk5LTYyMzE2NDNfcGluXzA   \n",
      "132788  758645  137299           7         MTM3Mjk5LTYyMzE2NDRfcGluXzA   \n",
      "132789  758646  137299           0         MTM3Mjk5LTYyMzE2NDVfcGluXzA   \n",
      "132790  758657  137299           6         MTM3Mjk5LTYyMzE2NDZfcGluXzA   \n",
      "132791  758713  137299           3         MTM3Mjk5LTYyMzE0MjBfcGluXzA   \n",
      "132792  758724  137299           3         MTM3Mjk5LTYyMzExMTJfcGluXzA   \n",
      "132793  758730  137299          58         MTM3Mjk5LTYyMzExMTNfcGluXzA   \n",
      "132794  758735  137299           7         MTM3Mjk5LTYyMzExMTRfcGluXzA   \n",
      "132795  758739  137299           6         MTM3Mjk5LTYyMzExMTVfcGluXzA   \n",
      "132796  758742  137299           8         MTM3Mjk5LTYyMzExMTZfcGluXzA   \n",
      "132797  758750  137299           7         MTM3Mjk5LTYyMzExMTdfcGluXzA   \n",
      "132798  758752  137299           9         MTM3Mjk5LTYyMzExMThfcGluXzA   \n",
      "132799  758762  137299           5         MTM3Mjk5LTYyMzExMTlfcGluXzA   \n",
      "\n",
      "          impact                                  share_token  \\\n",
      "0            NaN  nJ1zxu1n4vEROp9z02MmMkgCRRNvsW1sF16YTlQj-6Y   \n",
      "1            NaN  6P9SNi_tWcKqjGB4ap8VdkgCRRNvsW1sF16YTlQj-6Y   \n",
      "2            NaN  Q8khCIaSqAF9gdPlYn-osEgCRRNvsW1sF16YTlQj-6Y   \n",
      "3            NaN  Ac1X8s6131LIL5F8C70d_EgCRRNvsW1sF16YTlQj-6Y   \n",
      "4       0.415996  CgTJvMRNZ3iRq577UN-2BUgCRRNvsW1sF16YTlQj-6Y   \n",
      "5       0.000000  S-EVNPiurowt_mqhSxM7nUgCRRNvsW1sF16YTlQj-6Y   \n",
      "6       0.000000  yBxiI8eTwGddpxKzJTdDi0gCRRNvsW1sF16YTlQj-6Y   \n",
      "7       0.000000  6BKq4cPjuV5gUak9hUYj4UgCRRNvsW1sF16YTlQj-6Y   \n",
      "8       0.000000  kBjNyG9QC5UYNUgXmXJYhEgCRRNvsW1sF16YTlQj-6Y   \n",
      "9       0.000000  cPMuGBppz7-ANjyyTKIGH0gCRRNvsW1sF16YTlQj-6Y   \n",
      "10      1.039990  JKvJ1xuRG72s9fv0o3vAJUgCRRNvsW1sF16YTlQj-6Y   \n",
      "11      0.000000  ZAkFQnMZsuU7T91SL7XVfUgCRRNvsW1sF16YTlQj-6Y   \n",
      "12      0.000000  ru3HOg5pRQ17KBvs9HNgGkgCRRNvsW1sF16YTlQj-6Y   \n",
      "13      0.000000  jQGdop6SN_dHVulTm2SvLEgCRRNvsW1sF16YTlQj-6Y   \n",
      "14      0.000000  iTGquhiFPMoVv5RNDZpnBUgCRRNvsW1sF16YTlQj-6Y   \n",
      "15      0.000000  AOdifXG37gdvmbd1EVGHrUgCRRNvsW1sF16YTlQj-6Y   \n",
      "16      0.623994  tLEVR6PFLDl65uZADKxi20gCRRNvsW1sF16YTlQj-6Y   \n",
      "17      0.000000  gqToP5uQnL_n6odPQ3zhfkgCRRNvsW1sF16YTlQj-6Y   \n",
      "18      4.700436  DvH-145qrYNn8cV0Ev30AkgCRRNvsW1sF16YTlQj-6Y   \n",
      "19      0.000000  sEomBMQ96BwypEuMsfRtdUgCRRNvsW1sF16YTlQj-6Y   \n",
      "20      0.000000  3yGCPNkvBZJIblD6O7zYdEgCRRNvsW1sF16YTlQj-6Y   \n",
      "21      0.000000  12KPYnL6XuDfPwjQppHqJEgCRRNvsW1sF16YTlQj-6Y   \n",
      "22      0.000000  pWL98RHTJsXem5ZWsZDdl0gCRRNvsW1sF16YTlQj-6Y   \n",
      "23      0.831992  N2K3TScG2DxDY-718GY3PEgCRRNvsW1sF16YTlQj-6Y   \n",
      "24      0.000000  7dme9pZ_V2vXx4IomPqObUgCRRNvsW1sF16YTlQj-6Y   \n",
      "25      0.000000  4q030W649D4KvVgEbivVh0gCRRNvsW1sF16YTlQj-6Y   \n",
      "26      0.000000  vBmVhnPg-whFOrsy5rlA9UgCRRNvsW1sF16YTlQj-6Y   \n",
      "27      0.000000  kX4DXvAz5kMgA4ODqyIJsUgCRRNvsW1sF16YTlQj-6Y   \n",
      "28      0.000000  FP5tiZ-vTjcG62NZsBihf0gCRRNvsW1sF16YTlQj-6Y   \n",
      "29      0.000000  qQb7WGv8HTlXwCREaLEd3UgCRRNvsW1sF16YTlQj-6Y   \n",
      "...          ...                                          ...   \n",
      "132770  0.392081  JZO-u0aMKFQqidhfJ9HlcY_W39PeIcPreJizbaupgME   \n",
      "132771  0.130694  gPOBLCZrFaRR8WEDFUt-zY_W39PeIcPreJizbaupgME   \n",
      "132772  0.522774  wQhcnipcGvdRecOuWu8vWo_W39PeIcPreJizbaupgME   \n",
      "132773  0.261387  7lkyzUVRVj8CPlr3JuXpQI_W39PeIcPreJizbaupgME   \n",
      "132774  5.881211  nZWb-4pxqAINY9WeWu-aHI_W39PeIcPreJizbaupgME   \n",
      "132775  1.437629  0u064tRNT9CXPfKisUcUjY_W39PeIcPreJizbaupgME   \n",
      "132776  0.914855  MV3nQ3hAvmyri-A9PHT0UI_W39PeIcPreJizbaupgME   \n",
      "132777  0.130694  2TFYi5aEtusc_UGF-6XC24_W39PeIcPreJizbaupgME   \n",
      "132778  0.653468  bscnh3ttoOpwmiNcYt4lK4_W39PeIcPreJizbaupgME   \n",
      "132779  0.653468  PMoxwZYXXPwfONlsZkG0k4_W39PeIcPreJizbaupgME   \n",
      "132780  3.005952  H1zv_a8BBH3eNwc-K_CxK4_W39PeIcPreJizbaupgME   \n",
      "132781  3.398033  aXAyJUH4xvuPxA18LxeGgo_W39PeIcPreJizbaupgME   \n",
      "132782  0.392081  IM_eqrEzvxnJsMm23MbseY_W39PeIcPreJizbaupgME   \n",
      "132783  0.522774  Pvt5u4FTS54y_mWbK29pWI_W39PeIcPreJizbaupgME   \n",
      "132784  0.261387  M0WQ8UhGSNhLFnkrlmApqY_W39PeIcPreJizbaupgME   \n",
      "132785  0.522774  f8z8Onm194a7DMWBpfTn2Y_W39PeIcPreJizbaupgME   \n",
      "132786  0.130694  z7xZRrHU9UntD1H4lUk0g4_W39PeIcPreJizbaupgME   \n",
      "132787  0.000000  jSwABT8kVkP-LUyMuj51_I_W39PeIcPreJizbaupgME   \n",
      "132788  0.914855  LQso2zWXSX0a2U_i7x1_PI_W39PeIcPreJizbaupgME   \n",
      "132789  0.000000  9U1UzUw0nZfrj48UUsBExI_W39PeIcPreJizbaupgME   \n",
      "132790  0.784161  InW5K4MMRFqAfE06YPNSKI_W39PeIcPreJizbaupgME   \n",
      "132791  0.392081  -EheG4xTsvavvuDnD0_vzI_W39PeIcPreJizbaupgME   \n",
      "132792  0.392081  pBkLwQh10i0TNM0sDr8K1I_W39PeIcPreJizbaupgME   \n",
      "132793  7.580227  4h_-R6_cq6a8aIk1bb91Y4_W39PeIcPreJizbaupgME   \n",
      "132794  0.914855  qCAzfy8nUO6QJBdbxwDghY_W39PeIcPreJizbaupgME   \n",
      "132795  0.784161  aPE5TNrmsMNaGK8xLmsD_o_W39PeIcPreJizbaupgME   \n",
      "132796  1.045549  -eRE62LCJFdplSz-MokoKI_W39PeIcPreJizbaupgME   \n",
      "132797  0.914855  WIGpJcFNuXqxSYkLFtjgdY_W39PeIcPreJizbaupgME   \n",
      "132798  1.176242  Ez7d5n0DZw9wdx4puVAn7Y_W39PeIcPreJizbaupgME   \n",
      "132799  0.653468  pBRecUrnG2pY0NMh4rUMhY_W39PeIcPreJizbaupgME   \n",
      "\n",
      "                 timestamp type urls    channel     ...       \\\n",
      "0      2017-11-16 04:56:16  pin  NaN  pinterest     ...        \n",
      "1      2017-11-16 04:47:13  pin  NaN  pinterest     ...        \n",
      "2      2017-11-16 04:40:33  pin  NaN  pinterest     ...        \n",
      "3      2017-11-16 04:35:17  pin  NaN  pinterest     ...        \n",
      "4      2017-11-16 04:30:02  pin  NaN  pinterest     ...        \n",
      "5      2017-11-16 04:27:19  pin  NaN  pinterest     ...        \n",
      "6      2017-11-16 04:19:12  pin  NaN  pinterest     ...        \n",
      "7      2017-11-16 04:10:18  pin  NaN  pinterest     ...        \n",
      "8      2017-11-16 04:05:15  pin  NaN  pinterest     ...        \n",
      "9      2017-11-16 04:02:22  pin  NaN  pinterest     ...        \n",
      "10     2017-11-16 04:00:53  pin  NaN  pinterest     ...        \n",
      "11     2017-11-16 03:59:13  pin  NaN  pinterest     ...        \n",
      "12     2017-11-16 03:52:19  pin  NaN  pinterest     ...        \n",
      "13     2017-11-16 03:46:15  pin  NaN  pinterest     ...        \n",
      "14     2017-11-16 03:45:28  pin  NaN  pinterest     ...        \n",
      "15     2017-11-16 03:36:18  pin  NaN  pinterest     ...        \n",
      "16     2017-11-16 03:30:01  pin  NaN  pinterest     ...        \n",
      "17     2017-11-16 03:27:15  pin  NaN  pinterest     ...        \n",
      "18     2017-11-16 03:27:15  pin  NaN  pinterest     ...        \n",
      "19     2017-11-16 03:19:14  pin  NaN  pinterest     ...        \n",
      "20     2017-11-16 03:17:23  pin  NaN  pinterest     ...        \n",
      "21     2017-11-16 03:09:14  pin  NaN  pinterest     ...        \n",
      "22     2017-11-16 03:02:19  pin  NaN  pinterest     ...        \n",
      "23     2017-11-16 03:00:57  pin  NaN  pinterest     ...        \n",
      "24     2017-11-16 02:59:18  pin  NaN  pinterest     ...        \n",
      "25     2017-11-16 02:55:26  pin  NaN  pinterest     ...        \n",
      "26     2017-11-16 02:50:23  pin  NaN  pinterest     ...        \n",
      "27     2017-11-16 02:47:09  pin  NaN  pinterest     ...        \n",
      "28     2017-11-16 02:44:15  pin  NaN  pinterest     ...        \n",
      "29     2017-11-16 02:34:14  pin  NaN  pinterest     ...        \n",
      "...                    ...  ...  ...        ...     ...        \n",
      "132770 2016-01-01 16:27:13  pin  NaN  pinterest     ...        \n",
      "132771 2016-01-01 16:19:16  pin  NaN  pinterest     ...        \n",
      "132772 2016-01-01 15:58:10  pin  NaN  pinterest     ...        \n",
      "132773 2016-01-01 15:36:07  pin  NaN  pinterest     ...        \n",
      "132774 2016-01-01 15:19:10  pin  NaN  pinterest     ...        \n",
      "132775 2016-01-01 15:17:09  pin  NaN  pinterest     ...        \n",
      "132776 2016-01-01 15:01:17  pin  NaN  pinterest     ...        \n",
      "132777 2016-01-01 14:55:08  pin  NaN  pinterest     ...        \n",
      "132778 2016-01-01 14:43:07  pin  NaN  pinterest     ...        \n",
      "132779 2016-01-01 14:39:08  pin  NaN  pinterest     ...        \n",
      "132780 2016-01-01 14:36:09  pin  NaN  pinterest     ...        \n",
      "132781 2016-01-01 14:18:15  pin  NaN  pinterest     ...        \n",
      "132782 2016-01-01 14:05:11  pin  NaN  pinterest     ...        \n",
      "132783 2016-01-01 13:58:12  pin  NaN  pinterest     ...        \n",
      "132784 2016-01-01 13:55:05  pin  NaN  pinterest     ...        \n",
      "132785 2016-01-01 13:32:06  pin  NaN  pinterest     ...        \n",
      "132786 2016-01-01 13:16:08  pin  NaN  pinterest     ...        \n",
      "132787 2016-01-01 13:00:16  pin  NaN  pinterest     ...        \n",
      "132788 2016-01-01 12:55:09  pin  NaN  pinterest     ...        \n",
      "132789 2016-01-01 12:50:22  pin  NaN  pinterest     ...        \n",
      "132790 2016-01-01 12:20:06  pin  NaN  pinterest     ...        \n",
      "132791 2016-01-01 07:59:06  pin  NaN  pinterest     ...        \n",
      "132792 2016-01-01 07:06:08  pin  NaN  pinterest     ...        \n",
      "132793 2016-01-01 06:40:06  pin  NaN  pinterest     ...        \n",
      "132794 2016-01-01 06:12:06  pin  NaN  pinterest     ...        \n",
      "132795 2016-01-01 06:01:53  pin  NaN  pinterest     ...        \n",
      "132796 2016-01-01 05:44:08  pin  NaN  pinterest     ...        \n",
      "132797 2016-01-01 05:23:03  pin  NaN  pinterest     ...        \n",
      "132798 2016-01-01 05:14:03  pin  NaN  pinterest     ...        \n",
      "132799 2016-01-01 05:00:17  pin  NaN  pinterest     ...        \n",
      "\n",
      "                    pin_id                                       pin_url  \\\n",
      "0       246290673355306588  http://pinterest.com/pin/246290673355306588/   \n",
      "1       246290673355306537  http://pinterest.com/pin/246290673355306537/   \n",
      "2       246290673355306511  http://pinterest.com/pin/246290673355306511/   \n",
      "3       246290673355306491  http://pinterest.com/pin/246290673355306491/   \n",
      "4       215821007127568235  http://pinterest.com/pin/215821007127568235/   \n",
      "5       246290673355306466  http://pinterest.com/pin/246290673355306466/   \n",
      "6       246290673355306426  http://pinterest.com/pin/246290673355306426/   \n",
      "7       246290673355306354  http://pinterest.com/pin/246290673355306354/   \n",
      "8       246290673355306323  http://pinterest.com/pin/246290673355306323/   \n",
      "9       246290673355306303  http://pinterest.com/pin/246290673355306303/   \n",
      "10      215821007127568130  http://pinterest.com/pin/215821007127568130/   \n",
      "11      246290673355306286  http://pinterest.com/pin/246290673355306286/   \n",
      "12      246290673355306240  http://pinterest.com/pin/246290673355306240/   \n",
      "13      246290673355306214  http://pinterest.com/pin/246290673355306214/   \n",
      "14      246290673355306210  http://pinterest.com/pin/246290673355306210/   \n",
      "15      246290673355306155  http://pinterest.com/pin/246290673355306155/   \n",
      "16      215821007127567953  http://pinterest.com/pin/215821007127567953/   \n",
      "17      246290673355306126  http://pinterest.com/pin/246290673355306126/   \n",
      "18      200691727128412098  http://pinterest.com/pin/200691727128412098/   \n",
      "19      246290673355306113  http://pinterest.com/pin/246290673355306113/   \n",
      "20      246290673355306105  http://pinterest.com/pin/246290673355306105/   \n",
      "21      246290673355306046  http://pinterest.com/pin/246290673355306046/   \n",
      "22      246290673355306010  http://pinterest.com/pin/246290673355306010/   \n",
      "23      215821007127567808  http://pinterest.com/pin/215821007127567808/   \n",
      "24      246290673355305998  http://pinterest.com/pin/246290673355305998/   \n",
      "25      246290673355305987  http://pinterest.com/pin/246290673355305987/   \n",
      "26      246290673355305974  http://pinterest.com/pin/246290673355305974/   \n",
      "27      246290673355305955  http://pinterest.com/pin/246290673355305955/   \n",
      "28      106538347419211128  http://pinterest.com/pin/106538347419211128/   \n",
      "29      246290673355305893  http://pinterest.com/pin/246290673355305893/   \n",
      "...                    ...                                           ...   \n",
      "132770  246290673349660883  http://pinterest.com/pin/246290673349660883/   \n",
      "132771  246290673349660860  http://pinterest.com/pin/246290673349660860/   \n",
      "132772  246290673349660775  http://pinterest.com/pin/246290673349660775/   \n",
      "132773  246290673349660726  http://pinterest.com/pin/246290673349660726/   \n",
      "132774   91620173647959247   http://pinterest.com/pin/91620173647959247/   \n",
      "132775  246290673349660655  http://pinterest.com/pin/246290673349660655/   \n",
      "132776  246290673349660578  http://pinterest.com/pin/246290673349660578/   \n",
      "132777   22306960632463409   http://pinterest.com/pin/22306960632463409/   \n",
      "132778  246290673349660524  http://pinterest.com/pin/246290673349660524/   \n",
      "132779  246290673349660514  http://pinterest.com/pin/246290673349660514/   \n",
      "132780  246290673349660505  http://pinterest.com/pin/246290673349660505/   \n",
      "132781  259027416046348618  http://pinterest.com/pin/259027416046348618/   \n",
      "132782  246290673349660421  http://pinterest.com/pin/246290673349660421/   \n",
      "132783  246290673349660408  http://pinterest.com/pin/246290673349660408/   \n",
      "132784  246290673349660403  http://pinterest.com/pin/246290673349660403/   \n",
      "132785  246290673349660389  http://pinterest.com/pin/246290673349660389/   \n",
      "132786   22306960632463212   http://pinterest.com/pin/22306960632463212/   \n",
      "132787  246290673349660356  http://pinterest.com/pin/246290673349660356/   \n",
      "132788  246290673349660353  http://pinterest.com/pin/246290673349660353/   \n",
      "132789  246290673349660349  http://pinterest.com/pin/246290673349660349/   \n",
      "132790  246290673349660331  http://pinterest.com/pin/246290673349660331/   \n",
      "132791  246290673349659994  http://pinterest.com/pin/246290673349659994/   \n",
      "132792  246290673349659902  http://pinterest.com/pin/246290673349659902/   \n",
      "132793   91620173647957792   http://pinterest.com/pin/91620173647957792/   \n",
      "132794  246290673349659797  http://pinterest.com/pin/246290673349659797/   \n",
      "132795  246290673349659770  http://pinterest.com/pin/246290673349659770/   \n",
      "132796  246290673349659717  http://pinterest.com/pin/246290673349659717/   \n",
      "132797  246290673349659672  http://pinterest.com/pin/246290673349659672/   \n",
      "132798  246290673349659638  http://pinterest.com/pin/246290673349659638/   \n",
      "132799  246290673349659608  http://pinterest.com/pin/246290673349659608/   \n",
      "\n",
      "        pin_count post_type  repin_count  summary  thumbnail_url title  \\\n",
      "0             NaN   article          0.0      NaN            NaN   NaN   \n",
      "1             NaN   article          1.0      NaN            NaN   NaN   \n",
      "2             NaN   article          0.0      NaN            NaN   NaN   \n",
      "3             NaN   article          0.0      NaN            NaN   NaN   \n",
      "4             NaN   article          4.0      NaN            NaN   NaN   \n",
      "5             NaN   article          0.0      NaN            NaN   NaN   \n",
      "6             NaN   article          0.0      NaN            NaN   NaN   \n",
      "7             NaN   article          1.0      NaN            NaN   NaN   \n",
      "8             NaN   article          0.0      NaN            NaN   NaN   \n",
      "9             NaN   article          0.0      NaN            NaN   NaN   \n",
      "10            NaN   article         10.0      NaN            NaN   NaN   \n",
      "11            NaN   article          1.0      NaN            NaN   NaN   \n",
      "12            NaN   article          0.0      NaN            NaN   NaN   \n",
      "13            NaN   article          0.0      NaN            NaN   NaN   \n",
      "14            NaN   article          2.0      NaN            NaN   NaN   \n",
      "15            NaN   article          0.0      NaN            NaN   NaN   \n",
      "16            NaN   article          6.0      NaN            NaN   NaN   \n",
      "17            NaN   article          0.0      NaN            NaN   NaN   \n",
      "18            NaN   article         11.0      NaN            NaN   NaN   \n",
      "19            NaN   article          0.0      NaN            NaN   NaN   \n",
      "20            NaN   article          0.0      NaN            NaN   NaN   \n",
      "21            NaN   article          0.0      NaN            NaN   NaN   \n",
      "22            NaN   article          0.0      NaN            NaN   NaN   \n",
      "23            NaN   article          8.0      NaN            NaN   NaN   \n",
      "24            NaN   article          0.0      NaN            NaN   NaN   \n",
      "25            NaN   article          0.0      NaN            NaN   NaN   \n",
      "26            NaN   article          0.0      NaN            NaN   NaN   \n",
      "27            NaN   article          0.0      NaN            NaN   NaN   \n",
      "28            NaN   article          0.0      NaN            NaN   NaN   \n",
      "29            NaN   article          0.0      NaN            NaN   NaN   \n",
      "...           ...       ...          ...      ...            ...   ...   \n",
      "132770        NaN   article          1.0      NaN            NaN   NaN   \n",
      "132771        NaN   article          1.0      NaN            NaN   NaN   \n",
      "132772        NaN   article          4.0      NaN            NaN   NaN   \n",
      "132773        NaN   article          1.0      NaN            NaN   NaN   \n",
      "132774        NaN   article         37.0      NaN            NaN   NaN   \n",
      "132775        NaN   article          7.0      NaN            NaN   NaN   \n",
      "132776        NaN   article          4.0      NaN            NaN   NaN   \n",
      "132777        NaN   article          0.0      NaN            NaN   NaN   \n",
      "132778        NaN   article          2.0      NaN            NaN   NaN   \n",
      "132779        NaN   article          4.0      NaN            NaN   NaN   \n",
      "132780        NaN   article         17.0      NaN            NaN   NaN   \n",
      "132781        NaN     photo         19.0      NaN            NaN   NaN   \n",
      "132782        NaN   article          3.0      NaN            NaN   NaN   \n",
      "132783        NaN   article          2.0      NaN            NaN   NaN   \n",
      "132784        NaN   article          1.0      NaN            NaN   NaN   \n",
      "132785        NaN   article          3.0      NaN            NaN   NaN   \n",
      "132786        NaN   article          1.0      NaN            NaN   NaN   \n",
      "132787        NaN   article          0.0      NaN            NaN   NaN   \n",
      "132788        NaN   article          5.0      NaN            NaN   NaN   \n",
      "132789        NaN   article          0.0      NaN            NaN   NaN   \n",
      "132790        NaN   article          3.0      NaN            NaN   NaN   \n",
      "132791        NaN   article          0.0      NaN            NaN   NaN   \n",
      "132792        NaN   article          2.0      NaN            NaN   NaN   \n",
      "132793        NaN   article         43.0      NaN            NaN   NaN   \n",
      "132794        NaN   article          5.0      NaN            NaN   NaN   \n",
      "132795        NaN   article          3.0      NaN            NaN   NaN   \n",
      "132796        NaN   article          5.0      NaN            NaN   NaN   \n",
      "132797        NaN   article          3.0      NaN            NaN   NaN   \n",
      "132798        NaN   article          5.0      NaN            NaN   NaN   \n",
      "132799        NaN   article          5.0      NaN            NaN   NaN   \n",
      "\n",
      "       tweet_count  links_count  \n",
      "0              NaN            0  \n",
      "1              NaN            0  \n",
      "2              NaN            0  \n",
      "3              NaN            0  \n",
      "4              NaN            0  \n",
      "5              NaN            0  \n",
      "6              NaN            0  \n",
      "7              NaN            0  \n",
      "8              NaN            0  \n",
      "9              NaN            0  \n",
      "10             NaN            0  \n",
      "11             NaN            0  \n",
      "12             NaN            0  \n",
      "13             NaN            0  \n",
      "14             NaN            0  \n",
      "15             NaN            0  \n",
      "16             NaN            0  \n",
      "17             NaN            0  \n",
      "18             NaN            0  \n",
      "19             NaN            0  \n",
      "20             NaN            0  \n",
      "21             NaN            0  \n",
      "22             NaN            0  \n",
      "23             NaN            0  \n",
      "24             NaN            0  \n",
      "25             NaN            0  \n",
      "26             NaN            0  \n",
      "27             NaN            0  \n",
      "28             NaN            0  \n",
      "29             NaN            0  \n",
      "...            ...          ...  \n",
      "132770         NaN            1  \n",
      "132771         NaN            1  \n",
      "132772         NaN            1  \n",
      "132773         NaN            1  \n",
      "132774         NaN            1  \n",
      "132775         NaN            1  \n",
      "132776         NaN            1  \n",
      "132777         NaN            1  \n",
      "132778         NaN            1  \n",
      "132779         NaN            1  \n",
      "132780         NaN            1  \n",
      "132781         NaN            1  \n",
      "132782         NaN            1  \n",
      "132783         NaN            1  \n",
      "132784         NaN            1  \n",
      "132785         NaN            1  \n",
      "132786         NaN            1  \n",
      "132787         NaN            1  \n",
      "132788         NaN            1  \n",
      "132789         NaN            1  \n",
      "132790         NaN            1  \n",
      "132791         NaN            1  \n",
      "132792         NaN            1  \n",
      "132793         NaN            1  \n",
      "132794         NaN            1  \n",
      "132795         NaN            1  \n",
      "132796         NaN            1  \n",
      "132797         NaN            1  \n",
      "132798         NaN            1  \n",
      "132799         NaN            1  \n",
      "\n",
      "[132800 rows x 33 columns]**...\n",
      "Removing stop words...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Word Removal Complete.\n",
      "Extracting other attributes from titles...\n",
      "Creating column for best\n",
      "Creating column for sex\n",
      "Creating column for now\n",
      "Creating column for new\n",
      "Creating column for episode\n",
      "Creating column for how\n",
      "Title Feature Extraction Complete\n",
      "Initializing Topic Modeling...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.054*\"a\" + 0.051*\"the\" + 0.032*\"in\" + 0.027*\"and\" + 0.025*\"of\" + 0.023*\"|\" + 0.016*\"archdigest.com\" + 0.014*\"by\" + 0.012*\"home\" + 0.011*\"with\" + 0.010*\"is\" + 0.008*\"room\" + 0.007*\"an\" + 0.007*\"on\" + 0.006*\"from\" + 0.006*\"kitchen\" + 0.006*\"this\" + 0.006*\"for\" + 0.006*\"new\" + 0.006*\"to\" + 0.005*\"at\" + 0.005*\"designer\" + 0.004*\"house\" + 0.004*\"design\" + 0.003*\"york\" + 0.003*\"are\" + 0.003*\"space\" + 0.003*\"master\" + 0.003*\"living\" + 0.003*\"feature\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.097*\"see\" + 0.071*\"spring\" + 0.068*\"fall\" + 0.066*\"the\" + 0.053*\"-wmag\" + 0.050*\"collection\" + 0.048*\"from\" + 0.028*\"couture\" + 0.016*\"2017\" + 0.014*\"every\" + 0.013*\"show\" + 0.011*\"view\" + 0.011*\"complete\" + 0.010*\"pre-fall\" + 0.009*\"&\" + 0.009*\"chanel\" + 0.008*\"detailed\" + 0.008*\"alexander\" + 0.007*\"resort\" + 0.007*\"full\" + 0.006*\"dolce\" + 0.006*\"gabbana\" + 0.006*\"dior\" + 0.006*\"valli\" + 0.006*\"giambattista\" + 0.005*\"mcqueen\" + 0.005*\"haute\" + 0.005*\"entire\" + 0.005*\"de\" + 0.005*\"givenchy\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.131*\"allure.com\" + 0.130*\"|\" + 0.042*\"2016\" + 0.024*\"at\" + 0.021*\"the\" + 0.021*\"fashion\" + 0.020*\"look\" + 0.017*\"best\" + 0.016*\"style\" + 0.014*\"week\" + 0.014*\"new\" + 0.014*\"beauty\" + 0.013*\"from\" + 0.012*\"street\" + 0.010*\"in\" + 0.008*\"and\" + 0.007*\"photo\" + 0.007*\"show\" + 0.007*\"by\" + 0.006*\"backstage\" + 0.006*\"of\" + 0.006*\"day\" + 0.006*\"men\" + 0.005*\"vogue\" + 0.005*\"york\" + 0.005*\"hair\" + 0.005*\"outfit\" + 0.005*\"london\" + 0.005*\"paris\" + 0.004*\"more\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.057*\"the\" + 0.023*\"and\" + 0.022*\"of\" + 0.020*\"in\" + 0.015*\"to\" + 0.015*\"a\" + 0.012*\"for\" + 0.012*\"on\" + 0.012*\"with\" + 0.010*\"her\" + 0.008*\"is\" + 0.008*\"most\" + 0.007*\"lipstick\" + 0.007*\"hair\" + 0.007*\"lip\" + 0.007*\"red\" + 0.007*\"makeup\" + 0.006*\"best\" + 0.006*\"2015\" + 0.006*\"dress\" + 0.006*\"trend\" + 0.006*\"all\" + 0.006*\"look\" + 0.005*\"new\" + 0.005*\"this\" + 0.005*\"we\" + 0.005*\"beautiful\" + 0.005*\"nyfw\" + 0.004*\"it\" + 0.004*\"black\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.049*\"to\" + 0.023*\"you\" + 0.022*\"and\" + 0.022*\"for\" + 0.020*\"a\" + 0.018*\"the\" + 0.017*\"your\" + 0.014*\"click\" + 0.013*\"how\" + 0.010*\"of\" + 0.010*\"that\" + 0.010*\"it\" + 0.010*\"this\" + 0.009*\"hair\" + 0.009*\"we\" + 0.007*\"is\" + 0.007*\"are\" + 0.006*\"with\" + 0.006*\"in\" + 0.006*\"make\" + 0.006*\"like\" + 0.006*\"get\" + 0.006*\"more\" + 0.006*\"skin\" + 0.005*\"on\" + 0.005*\"can\" + 0.005*\"all\" + 0.005*\"here\" + 0.005*\"beauty\" + 0.005*\"these\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Adding Topic 0...\n",
      "Adding Topic 1...\n",
      "Adding Topic 2...\n",
      "Adding Topic 3...\n",
      "Adding Topic 4...\n",
      "Percetange Of Observations Missing Topic Values:\n",
      "28.1995481928\n"
     ]
    }
   ],
   "source": [
    "words = ['best','sex','now','new','episode','how']\n",
    "df_pin = early_pipeline(df_new, medium_type = 'pin', publication = None, list_of_words = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "celeb = \n",
    "['beyonce','kim','karsashian','taylor','swift','justin','bieber','rihanna','scarlet','johansson','dwayne','johnson',\n",
    " 'ellen','degeneres','katy','perry','angelina','jolie','drake','brad','pitt','jay','cristiano','ronaldo','jennifer',\n",
    " 'aniston','oprah','winfrey','adele','jonny','depp','tom','cruise','jennifer','lopez','sean','colms','jennifer','lawrence',\n",
    " 'leonardo','dicaprio','sandra','bullock','selena','gomez','tom','hanks','julia','roberts','howard','stern','donald',\n",
    " 'trump','robert','downey','britney','spears','adam','sandler','megan','fox','kylie','jenner','miley','cyrus','jessica',\n",
    " 'alba','emma','watson','eminem','paris','hilton','vin','diesel','kevin','hart','will','smith','chris','rock',\n",
    " 'chris','hemsworth','chris','pratt','ben','affleck','matt','damon','denzel','washington']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
