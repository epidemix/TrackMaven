{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('https://s3.amazonaws.com/temp-data-pulls/newdump.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "rawpin_blog = df[(df[\"type\"]==\"pin\") | (df[\"type\"]==\"blog post\")]\n",
    "rawpin_blog.drop([\"has_spend\"], axis = 1, inplace=True)\n",
    "channel_info = rawpin_blog['channel_info'].apply(pd.Series)\n",
    "channel_info.columns = [\"channel\", \"info\"]\n",
    "content_info = rawpin_blog['content'].apply(pd.Series)\n",
    "content_info.drop(['author_email', 'content', 'pinned_from'], axis=1, inplace=True) ## THESE HAVE ONLY NULLS\n",
    "for x in content_info.columns:\n",
    "    if \"count\" in x:\n",
    "        content_info[x].fillna(np.NaN, inplace = True)\n",
    "        #content_info[x] = content_info[x].astype(int)\n",
    "master_pinblog = rawpin_blog.join(channel_info).join(content_info)\n",
    "master_pinblog.drop(['channel_info', 'content'], axis = 1, inplace = True)\n",
    "master_pinblog.columns = ['brand', 'engagement', 'uniqueid', 'impact', 'share_token', 'timestamp',\n",
    "       'type', 'urls', 'channel', 'info', 'author_name', 'comment_count',\n",
    "       'description', 'fb_likecount', 'fb_sharecount',\n",
    "       'gplus_count', 'hashtags', 'image_url', 'like_count',\n",
    "       'link', 'linkedin_sharecount', 'links', 'pin_id', 'pin_url',\n",
    "       'pin_count', 'post_type', 'repin_count', 'summary',\n",
    "       'thumbnail_url', 'title', 'tweet_count']\n",
    "\n",
    "master_pinblog[\"links_count\"] = master_pinblog['links'].str.len()\n",
    "df_new = master_pinblog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = master_pinblog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.link = df_new.link.astype(str)\n",
    "df_new.link.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatizing give you the most basic form of a word\n",
    "# this function differs from one further along in the code because this one\n",
    "# keeps common english \n",
    "def lemmatizing_titles(pdseries):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #en_stop = get_stop_words('en') if not i in en_stop\n",
    "    # when a word is lemmatized, contractions are rightfully turned into different lems since 's = is\n",
    "    # however, in this case contractions shorten a title's lenth, so we don't want to count them separately\n",
    "    # question marks and the like shouldn't be counted as a separate word in a title\n",
    "    contractions = [\"'s\",\"s\",\"'\",\".\",\",\",\"n't\",\"'d\",\"ll\",\"re\",\"ve\",\"``\",\n",
    "                    \"''\",\"”\",\"“\",\"’\",\"(\",\")\",\"?\",\":\",\"t\",\";\",\"d\",\"!\",\"-\",\"[\",\"]\",\"w\",\"#\",\"m\"]\n",
    "    #other_words = [\"new\",\"get\",]\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    post_text = [i for i in pdseries]\n",
    "    count = 1\n",
    "    print(f\"Initializing tokenizer and lemmatizer ...\")\n",
    "    print(\"Number of titles tokenized and lemmatized:\")\n",
    "    for i in post_text:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "\n",
    "        # stem tokens and remove stop words\n",
    "        lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "\n",
    "        #remove stemmed contractions\n",
    "        contracted_tokens = [i for i in lemmed_tokens if not i in contractions]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(contracted_tokens)\n",
    "        if count % 5000 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    print(\"Lemmatizing Completed.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new df called blogs that only contains blogs\n",
    "blogs = df_new[df_new.type == 'blog post']\n",
    "blogs.reset_index(inplace = True)\n",
    "\n",
    "#create new df for pinterest\n",
    "pins = df_new[df_new.type == 'pin']\n",
    "pins.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing title feature extraction...\n",
      "Initializing word count for title length of **blogs**...\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of titles tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "Lemmatizing Completed.\n",
      "Initializing word count for title length of **pins**...\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of titles tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "Lemmatizing Completed.\n",
      "Extracting other attributes from titles...\n",
      "Title Feature Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "print('Initializing title feature extraction...')\n",
    "print('Initializing word count for title length of **blogs**...')\n",
    "# need to tokenize and lemmatize to count the length\n",
    "lemmatized_titles = lemmatizing_titles(blogs.title)\n",
    "# adding to dataframe\n",
    "blogs['title_length'] = [len(lemmatized_titles[i]) for i in range(len(lemmatized_titles))]\n",
    "\n",
    "print('Initializing word count for title length of **pins**...')\n",
    "# doing the same for pinterest, those are titles in description\n",
    "lemmatized_descriptions = lemmatizing_titles(pins.description)\n",
    "# adding to dataframe\n",
    "pins['title_length'] = [len(lemmatized_descriptions[i]) for i in range(len(lemmatized_descriptions))]\n",
    "\n",
    "print('Extracting other attributes from titles...')\n",
    "# the following code is a bunch of different feature extractions for the titles\n",
    "blogs['title_is_question'] = ['?' in i for i in blogs.title]\n",
    "pins['title_is_question'] = ['?' in i for i in pins.description]\n",
    "\n",
    "blogs['title_contains_number'] = [any(x in i for x in ['1','2','3','4','5','6','7','8','9','0']) for i in blogs.title]\n",
    "pins['title_contains_number'] = [any(x in i for x in ['1','2','3','4','5','6','7','8','9','0']) for i in pins.description]\n",
    "\n",
    "blogs['title_contains_best'] = ['best' in i.lower() for i in blogs.title]\n",
    "pins['title_contains_best'] = ['best' in i.lower() for i in pins.description]\n",
    "blogs['title_contains_sex'] = ['sex' in i.lower() for i in blogs.title]\n",
    "pins['title_contains_sex'] = ['sex' in i.lower() for i in pins.description]\n",
    "blogs['title_contains_now'] = ['now' in i.lower() for i in blogs.title]\n",
    "pins['title_contains_now'] = ['now' in i.lower() for i in pins.description]\n",
    "blogs['title_contains_new'] = ['new' in i.lower() for i in blogs.title]\n",
    "pins['title_contains_new'] = ['new' in i.lower() for i in pins.description]\n",
    "blogs['title_contains_episode'] = ['episode' in i.lower() for i in blogs.title]\n",
    "pins['title_contains_episode'] = ['episode' in i.lower() for i in pins.description]\n",
    "\n",
    "print('Title Feature Extraction Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domains of Blogs:\n",
      "['glamour' 'teenvogue' 'wmagazine' 'allure' 'cntraveler'\n",
      " 'architecturaldigest' 'vogue']\n"
     ]
    }
   ],
   "source": [
    "# converts link to string so we can split\n",
    "blogs.link = blogs.link.astype(str)\n",
    "# instantiate a new list called new_mag\n",
    "new_mag = []\n",
    "# list comprehension that just keeps part before '.com'\n",
    "# we can use list comprehension because this is true for all values\n",
    "magazine = [i.split('.com')[0] for i in blogs.link]\n",
    "# start for loop to get rid of everything before the name of the magazine\n",
    "for i in magazine:\n",
    "    if '.' in i:\n",
    "        new_mag.append(i.split('.')[1])\n",
    "# if there isn't a '.' it just sends the existing name to the list\n",
    "    else:\n",
    "        new_mag.append(i)\n",
    "# create new column for the blog df with the publications\n",
    "blogs['pub'] = new_mag\n",
    "\n",
    "# create a new subset of dataframes based on publication\n",
    "df_blogs_glamour = blogs[blogs.pub == 'glamour']\n",
    "#df_pins_glamour = pins[pins.pub == 'glamour']\n",
    "df_blogs_teenvogue = blogs[blogs.pub == 'teenvogue']\n",
    "#df_pins_teenvogue = pins[pins.pub == 'teenvogue']\n",
    "df_blogs_wmagazine = blogs[blogs.pub == 'wmagazine']\n",
    "#df_pins_wmagazine = pins[pins.pub == 'wmagazine']\n",
    "df_blogs_allure = blogs[blogs.pub == 'allure']\n",
    "#df_pins_allure = pins[pins.pub == 'allure']\n",
    "df_blogs_cntraveler = blogs[blogs.pub == 'cntraveler']\n",
    "#df_pins_cntraveler = pins[pins.pub == 'cntraveler']\n",
    "df_blogs_architecturaldigest = blogs[blogs.pub == 'architecturaldigest']\n",
    "#df_pins_architecturaldigest = pins[pins.pub == 'architecturaldigest']\n",
    "df_blogs_vogue = blogs[blogs.pub == 'vogue']\n",
    "#df_pins_vogue = pins[pins.pub == 'vogue']\n",
    "\n",
    "print('Domains of Blogs:')\n",
    "print(blogs.pub.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatizing(pdseries):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    en_stop = get_stop_words('en')\n",
    "    # when a word is lemmatized, contractions are rightfully turned into different stems since 's = is\n",
    "    # however, in reality, all of those words are themselves stop words, so I want to exclude them\n",
    "    # question marks and the like are not helpful for our purpose of figuring out potential categories\n",
    "    contractions = [\"'s\",\"s\",\"'\",\".\",\",\",\"n't\",\"'d\",\"ll\",\"re\",\"ve\",\"``\",\n",
    "                    \"''\",\"”\",\"“\",\"’\",\"(\",\")\",\"?\",\":\",\"t\",\";\",\"d\",\"!\",\"-\",\"[\",\"]\",\"w\",\"#\",\"m\"]\n",
    "    #other_words = [\"new\",\"get\",]\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    post_text = [i for i in pdseries]\n",
    "    count = 1\n",
    "    print(f\"Initializing tokenizer and lemmatizer ...\")\n",
    "    print(\"Number of posts tokenized and lemmatized:\")\n",
    "    for i in post_text:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "\n",
    "        # stem tokens and remove stop words\n",
    "        lemmed_tokens = [lemmatizer.lemmatize(i) for i in tokens if not i in en_stop]\n",
    "\n",
    "        #remove stemmed contractions\n",
    "        contracted_tokens = [i for i in lemmed_tokens if not i in contractions]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(contracted_tokens)\n",
    "        if count % 5000 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    print(\"Lemmatizing Completed.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this stemmer didn't end up being used, but it's good code if you\n",
    "# want to stem instead of lemmatize\n",
    "# stemming give you the root of a word\n",
    "# while lemmatizing give you the most basic form of a word\n",
    "def stemming(pdseries):\n",
    "    stemmer = SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "    en_stop = get_stop_words('en')\n",
    "    # when a word is stemmed, contractions are rightfully turned into different stems since 's = is\n",
    "    # however, in reality, all of those words are themselves stop words, so I want to exclude them\n",
    "    # question marks and the like are not helpful for our purpose of figuring out potential categories\n",
    "    contractions = [\"'s\",\"s\",\"'\",\".\",\",\",\"n't\",\"'d\",\"ll\",\"re\",\"ve\",\"``\",\n",
    "                    \"''\",\"”\",\"“\",\"’\",\"(\",\")\",\"?\",\":\",\"t\",\";\",\"d\",\"!\",\"-\",\"[\",\"]\",\"w\",\"#\",\"m\"]\n",
    "    #other_words = [\"new\",\"get\",]\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    post_text = [i for i in pdseries]\n",
    "    count = 1\n",
    "    print(f\"Initializing tokenizer and stemmer ...\")\n",
    "    print(\"Number of posts tokenized and stemmed:\")\n",
    "    for i in post_text:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "\n",
    "        # stem tokens and remove stop words\n",
    "        stemmed_tokens = [stemmer.stem(i) for i in tokens if not i in en_stop]\n",
    "\n",
    "        #remove stemmed contractions\n",
    "        contracted_tokens = [i for i in stemmed_tokens if not i in contractions]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(contracted_tokens)\n",
    "        if count % 5000 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "    print(\"Stemming Completed.\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "Lemmatizing Completed.\n",
      "Initializing tokenizer and lemmatizer ...\n",
      "Number of posts tokenized and lemmatized:\n",
      "5000\n",
      "10000\n",
      "Lemmatizing Completed.\n"
     ]
    }
   ],
   "source": [
    "glamour = lemmatizing(df_blogs_glamour.title)\n",
    "teenvogue = lemmatizing(df_blogs_teenvogue.title)\n",
    "wmagazine = lemmatizing(df_blogs_wmagazine.title)\n",
    "allure = lemmatizing(df_blogs_allure.title)\n",
    "cntraveler = lemmatizing(df_blogs_cntraveler.title)\n",
    "architecturaldigest = lemmatizing(df_blogs_architecturaldigest.title)\n",
    "vogue = lemmatizing(df_blogs_vogue.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to identify different groupings of words which count as topics\n",
    "def topic_modeling(pub,number_of_topics = 5,number_of_words = 30,number_of_passes = 1):\n",
    "    print(\"Initializing:...\")\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(pub)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in pub]\n",
    "    # generate LDA model\n",
    "    print(\"Generating Model...\")\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=number_of_topics, id2word = dictionary, passes=number_of_passes)\n",
    "    topics = ldamodel.print_topics(num_topics=number_of_topics, num_words=number_of_words)\n",
    "    print(\"Topics\\n\")\n",
    "    for i in range(number_of_topics):\n",
    "        print(f\"Topic {topics[i][0]}: \\n\")\n",
    "        print(topics[i][1], \"\\n\")\n",
    "    return ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***GLAMOUR***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.017*\"woman\" + 0.009*\"sex\" + 0.008*\"new\" + 0.007*\"will\" + 0.007*\"can\" + 0.007*\"thing\" + 0.007*\"day\" + 0.007*\"work\" + 0.007*\"season\" + 0.007*\"6\" + 0.007*\"get\" + 0.006*\"year\" + 0.005*\"make\" + 0.005*\"job\" + 0.005*\"7\" + 0.005*\"life\" + 0.005*\"5\" + 0.005*\"watch\" + 0.005*\"first\" + 0.005*\"wedding\" + 0.005*\"game\" + 0.005*\"much\" + 0.005*\"help\" + 0.005*\"beauty\" + 0.005*\"girl\" + 0.005*\"4\" + 0.004*\"gift\" + 0.004*\"8\" + 0.004*\"want\" + 0.004*\"recap\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.019*\"trump\" + 0.013*\"woman\" + 0.011*\"donald\" + 0.009*\"clinton\" + 0.008*\"hillary\" + 0.006*\"talk\" + 0.005*\"show\" + 0.005*\"sex\" + 0.005*\"new\" + 0.005*\"movie\" + 0.005*\"picture\" + 0.005*\"netflix\" + 0.005*\"just\" + 0.004*\"president\" + 0.004*\"know\" + 0.004*\"obama\" + 0.004*\"hadid\" + 0.004*\"say\" + 0.004*\"get\" + 0.004*\"coming\" + 0.004*\"will\" + 0.003*\"'re\" + 0.003*\"birth\" + 0.003*\"watch\" + 0.003*\"first\" + 0.003*\"abortion\" + 0.003*\"gigi\" + 0.003*\"need\" + 0.003*\"like\" + 0.003*\"bill\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.013*\"jenner\" + 0.011*\"new\" + 0.008*\"kylie\" + 0.008*\"just\" + 0.007*\"selena\" + 0.007*\"gomez\" + 0.007*\"kardashian\" + 0.006*\"kendall\" + 0.005*\"get\" + 0.005*\"like\" + 0.005*\"look\" + 0.005*\"bachelorette\" + 0.005*\"gala\" + 0.004*\"hair\" + 0.004*\"food\" + 0.004*\"kim\" + 0.004*\"taylor\" + 0.004*\"bachelor\" + 0.004*\"10\" + 0.004*\"$\" + 0.004*\"season\" + 0.004*\"swift\" + 0.004*\"woman\" + 0.004*\"way\" + 0.004*\"lady\" + 0.003*\"potter\" + 0.003*\"bang\" + 0.003*\"emma\" + 0.003*\"little\" + 0.003*\"big\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.008*\"new\" + 0.008*\"just\" + 0.006*\"woman\" + 0.006*\"photo\" + 0.006*\"kardashian\" + 0.005*\"perfect\" + 0.005*\"hair\" + 0.005*\"now\" + 0.005*\"relationship\" + 0.005*\"blake\" + 0.005*\"instagram\" + 0.004*\"hadid\" + 0.004*\"kim\" + 0.004*\"way\" + 0.004*\"lively\" + 0.004*\"starbucks\" + 0.004*\"right\" + 0.004*\"show\" + 0.004*\"ryan\" + 0.003*\"look\" + 0.003*\"can\" + 0.003*\"bella\" + 0.003*\"make\" + 0.003*\"two\" + 0.003*\"story\" + 0.003*\"finally\" + 0.003*\"west\" + 0.003*\"u\" + 0.003*\"real\" + 0.003*\"campaign\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.025*\"best\" + 0.023*\"2016\" + 0.018*\"summer\" + 0.009*\"makeup\" + 0.009*\"new\" + 0.009*\"dress\" + 0.009*\"outfit\" + 0.009*\"look\" + 0.008*\"hair\" + 0.008*\"trend\" + 0.008*\"style\" + 0.007*\"wear\" + 0.007*\"2017\" + 0.007*\"fashion\" + 0.007*\"idea\" + 0.007*\"celebrity\" + 0.006*\"wedding\" + 0.006*\"red\" + 0.006*\"taylor\" + 0.006*\"swift\" + 0.005*\"beauty\" + 0.005*\"star\" + 0.005*\"watch\" + 0.004*\"award\" + 0.004*\"music\" + 0.004*\"carpet\" + 0.004*\"see\" + 0.004*\"fall\" + 0.004*\"spring\" + 0.004*\"kate\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "0\n",
      "\n",
      "***TEENVOGUE***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.024*\"new\" + 0.019*\"taylor\" + 0.019*\"music\" + 0.018*\"swift\" + 0.013*\"video\" + 0.008*\"award\" + 0.008*\"just\" + 0.008*\"obama\" + 0.007*\"cyrus\" + 0.006*\"song\" + 0.006*\"year\" + 0.006*\"miley\" + 0.006*\"officially\" + 0.005*\"snapchat\" + 0.005*\"met\" + 0.005*\"liam\" + 0.005*\"watch\" + 0.005*\"york\" + 0.005*\"vogue\" + 0.004*\"gala\" + 0.004*\"tomlinson\" + 0.004*\"adele\" + 0.004*\"birthday\" + 0.004*\"major\" + 0.003*\"hemsworth\" + 0.003*\"really\" + 0.003*\"sander\" + 0.003*\"win\" + 0.003*\"louis\" + 0.003*\"list\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.018*\"2016\" + 0.014*\"trump\" + 0.013*\"one\" + 0.012*\"zayn\" + 0.010*\"new\" + 0.009*\"malik\" + 0.009*\"donald\" + 0.008*\"just\" + 0.008*\"—\" + 0.007*\"say\" + 0.007*\"woman\" + 0.006*\"jonas\" + 0.006*\"clinton\" + 0.006*\"hillary\" + 0.006*\"will\" + 0.005*\"now\" + 0.005*\"demi\" + 0.005*\"lovato\" + 0.005*\"make\" + 0.005*\"direction\" + 0.005*\"student\" + 0.004*\"way\" + 0.004*\"college\" + 0.004*\"nick\" + 0.004*\"want\" + 0.004*\"single\" + 0.004*\"show\" + 0.004*\"ever\" + 0.004*\"lady\" + 0.004*\"reason\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.015*\"new\" + 0.011*\"2016\" + 0.011*\"—\" + 0.010*\"hair\" + 0.010*\"movie\" + 0.009*\"need\" + 0.009*\"little\" + 0.009*\"girl\" + 0.008*\"star\" + 0.007*\"will\" + 0.007*\"prom\" + 0.006*\"thing\" + 0.006*\"10\" + 0.006*\"know\" + 0.006*\"trailer\" + 0.006*\"book\" + 0.005*\"season\" + 0.005*\"instagram\" + 0.005*\"best\" + 0.005*\"just\" + 0.005*\"emma\" + 0.005*\"teen\" + 0.004*\"liar\" + 0.004*\"tv\" + 0.004*\"‘\" + 0.004*\"makeup\" + 0.004*\"medium\" + 0.004*\"body\" + 0.004*\"social\" + 0.004*\"watch\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.025*\"hadid\" + 0.019*\"gigi\" + 0.016*\"justin\" + 0.016*\"bieber\" + 0.016*\"selena\" + 0.015*\"gomez\" + 0.014*\"wear\" + 0.011*\"instagram\" + 0.010*\"bella\" + 0.008*\"7\" + 0.008*\"new\" + 0.008*\"best\" + 0.007*\"dress\" + 0.006*\"just\" + 0.006*\"wore\" + 0.005*\"photo\" + 0.005*\"winter\" + 0.005*\"—\" + 0.005*\"day\" + 0.005*\"tour\" + 0.005*\"fashion\" + 0.005*\"$\" + 0.005*\"way\" + 0.004*\"beauty\" + 0.004*\"top\" + 0.004*\"spring\" + 0.004*\"20\" + 0.004*\"see\" + 0.003*\"read\" + 0.003*\"date\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.035*\"jenner\" + 0.021*\"kendall\" + 0.019*\"kylie\" + 0.012*\"beauty\" + 0.012*\"style\" + 0.011*\"hair\" + 0.010*\"fashion\" + 0.009*\"best\" + 0.008*\"harry\" + 0.007*\"look\" + 0.007*\"new\" + 0.007*\"red\" + 0.007*\"photo\" + 0.007*\"ariana\" + 0.006*\"grande\" + 0.006*\"show\" + 0.006*\"makeup\" + 0.006*\"carpet\" + 0.005*\"moment\" + 0.005*\"hailey\" + 0.005*\"baldwin\" + 0.005*\"cara\" + 0.005*\"school\" + 0.005*\"go\" + 0.004*\"viral\" + 0.004*\"delevingne\" + 0.004*\"see\" + 0.004*\"product\" + 0.004*\"super\" + 0.004*\"secret\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "0\n",
      "\n",
      "***WMAGAZINE***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.014*\"cate\" + 0.010*\"blanchett\" + 0.009*\"delevingne\" + 0.009*\"cara\" + 0.008*\"model\" + 0.008*\"show\" + 0.007*\"jean\" + 0.007*\"day\" + 0.007*\"couture\" + 0.007*\"party\" + 0.007*\"waterhouse\" + 0.007*\"suki\" + 0.007*\"new\" + 0.007*\"nyfw\" + 0.007*\"news\" + 0.007*\"jenner\" + 0.006*\"hollywood\" + 0.006*\"twitter\" + 0.005*\"chanel\" + 0.005*\"kendall\" + 0.005*\"tale\" + 0.005*\"meet\" + 0.005*\"fashion\" + 0.005*\"ready\" + 0.005*\"lbd\" + 0.005*\"celebrity\" + 0.005*\"face\" + 0.005*\"louis\" + 0.005*\"vuitton\" + 0.005*\"good\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.024*\"golden\" + 0.021*\"globe\" + 0.019*\"2016\" + 0.009*\"jennifer\" + 0.008*\"art\" + 0.007*\"slip\" + 0.007*\"february\" + 0.006*\"look\" + 0.006*\"get\" + 0.006*\"party\" + 0.006*\"london\" + 0.006*\"first\" + 0.005*\"inside\" + 0.005*\"beauty\" + 0.005*\"oscar\" + 0.005*\"new\" + 0.005*\"sarah\" + 0.005*\"alicia\" + 0.005*\"brie\" + 0.004*\"artist\" + 0.004*\"lopez\" + 0.004*\"larson\" + 0.004*\"lawrence\" + 0.004*\"win\" + 0.004*\"vikander\" + 0.004*\"debut\" + 0.004*\"club\" + 0.004*\"peter\" + 0.004*\"ten\" + 0.004*\"weekend\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.014*\"kate\" + 0.008*\"new\" + 0.008*\"bosworth\" + 0.008*\"think\" + 0.008*\"like\" + 0.007*\"$\" + 0.006*\"beauty\" + 0.006*\"america\" + 0.005*\"right\" + 0.005*\"might\" + 0.005*\"jolie\" + 0.005*\"angelina\" + 0.005*\"gucci\" + 0.005*\"david\" + 0.005*\"lover\" + 0.005*\"white\" + 0.005*\"war\" + 0.004*\"routine\" + 0.004*\"actual\" + 0.004*\"mia\" + 0.004*\"cat\" + 0.004*\"bowie\" + 0.004*\"pink\" + 0.004*\"robert\" + 0.004*\"smith\" + 0.004*\"swim\" + 0.004*\"valli\" + 0.004*\"anne\" + 0.004*\"fairy\" + 0.004*\"hathaway\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.025*\"fashion\" + 0.019*\"new\" + 0.015*\"week\" + 0.013*\"best\" + 0.011*\"york\" + 0.011*\"wear\" + 0.011*\"look\" + 0.010*\"west\" + 0.010*\"kanye\" + 0.009*\"girl\" + 0.008*\"jacob\" + 0.008*\"show\" + 0.007*\"style\" + 0.007*\"marc\" + 0.007*\"2017\" + 0.007*\"public\" + 0.006*\"nomination\" + 0.006*\"model\" + 0.006*\"beauty\" + 0.006*\"men\" + 0.006*\"like\" + 0.006*\"spring\" + 0.005*\"designer\" + 0.005*\"line\" + 0.005*\"oscar\" + 0.004*\"street\" + 0.004*\"kardashian\" + 0.004*\"los\" + 0.004*\"talk\" + 0.004*\"angeles\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.023*\"red\" + 0.018*\"carpet\" + 0.013*\"best\" + 0.009*\"dunst\" + 0.009*\"kirsten\" + 0.009*\"rosie\" + 0.009*\"shine\" + 0.008*\"sarah\" + 0.008*\"david\" + 0.008*\"lady\" + 0.008*\"everything\" + 0.008*\"trump\" + 0.007*\"art\" + 0.007*\"need\" + 0.006*\"know\" + 0.006*\"gwyneth\" + 0.006*\"paltrow\" + 0.006*\"celebrity\" + 0.006*\"beauty\" + 0.005*\"anderson\" + 0.005*\"jonathan\" + 0.005*\"performance\" + 0.005*\"smith\" + 0.005*\"charlize\" + 0.005*\"jaden\" + 0.005*\"lee\" + 0.005*\"donald\" + 0.005*\"playing\" + 0.005*\"shoe\" + 0.005*\"portrait\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "4\n",
      "\n",
      "***ALLURE***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.026*\"beauty\" + 0.016*\"product\" + 0.012*\"new\" + 0.007*\"woman\" + 0.007*\"get\" + 0.006*\"now\" + 0.006*\"lipstick\" + 0.006*\"can\" + 0.005*\"us\" + 0.005*\"10\" + 0.005*\"just\" + 0.005*\"kardashian\" + 0.005*\"right\" + 0.005*\"hack\" + 0.005*\"skin-care\" + 0.005*\"brand\" + 0.005*\"3\" + 0.004*\"face\" + 0.004*\"$\" + 0.004*\"make\" + 0.004*\"acne\" + 0.004*\"fashion\" + 0.004*\"secret\" + 0.004*\"sephora\" + 0.004*\"major\" + 0.004*\"will\" + 0.004*\"treatment\" + 0.004*\"study\" + 0.003*\"guide\" + 0.003*\"need\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.020*\"makeup\" + 0.020*\"new\" + 0.019*\"hair\" + 0.014*\"best\" + 0.010*\"trend\" + 0.010*\"beauty\" + 0.009*\"2017\" + 0.009*\"look\" + 0.008*\"way\" + 0.007*\"lipstick\" + 0.007*\"product\" + 0.006*\"thing\" + 0.006*\"5\" + 0.006*\"eye\" + 0.006*\"see\" + 0.006*\"instagram\" + 0.006*\"$\" + 0.006*\"collection\" + 0.005*\"fall\" + 0.005*\"highlighter\" + 0.005*\"favorite\" + 0.005*\"wear\" + 0.005*\"fashion\" + 0.005*\"drugstore\" + 0.005*\"every\" + 0.005*\"just\" + 0.005*\"line\" + 0.005*\"palette\" + 0.005*\"red\" + 0.004*\"color\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.022*\"beauty\" + 0.015*\"hair\" + 0.013*\"2016\" + 0.013*\"new\" + 0.011*\"just\" + 0.009*\"collection\" + 0.008*\"makeup\" + 0.008*\"best\" + 0.008*\"hadid\" + 0.007*\"look\" + 0.005*\"holiday\" + 0.005*\"secret\" + 0.005*\"kardashian\" + 0.005*\"hairstylist\" + 0.005*\"gigi\" + 0.005*\"urban\" + 0.005*\"need\" + 0.004*\"newest\" + 0.004*\"see\" + 0.004*\"nail\" + 0.004*\"mistake\" + 0.004*\"time\" + 0.004*\"decay\" + 0.004*\"bella\" + 0.004*\"product\" + 0.004*\"jenner\" + 0.004*\"woman\" + 0.004*\"know\" + 0.004*\"palette\" + 0.004*\"making\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.022*\"skin\" + 0.015*\"makeup\" + 0.011*\"look\" + 0.010*\"will\" + 0.009*\"review\" + 0.009*\"jenner\" + 0.009*\"need\" + 0.008*\"know\" + 0.007*\"make\" + 0.007*\"internet\" + 0.007*\"trick\" + 0.007*\"kylie\" + 0.005*\"everything\" + 0.005*\"eye\" + 0.005*\"like\" + 0.005*\"one\" + 0.005*\"give\" + 0.004*\"braid\" + 0.004*\"cream\" + 0.004*\"tone\" + 0.004*\"get\" + 0.004*\"kendall\" + 0.004*\"obsessed\" + 0.004*\"easy\" + 0.004*\"really\" + 0.004*\"swears\" + 0.004*\"artist\" + 0.004*\"better\" + 0.004*\"next\" + 0.003*\"product\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.013*\"hair\" + 0.012*\"woman\" + 0.010*\"lip\" + 0.007*\"makeup\" + 0.007*\"skin\" + 0.006*\"dress\" + 0.006*\"one\" + 0.006*\"will\" + 0.006*\"like\" + 0.005*\"kylie\" + 0.005*\"hack\" + 0.005*\"create\" + 0.005*\"genius\" + 0.005*\"make\" + 0.005*\"mask\" + 0.005*\"now\" + 0.005*\"reason\" + 0.004*\"probably\" + 0.004*\"wearing\" + 0.004*\"jennifer\" + 0.004*\"jenner\" + 0.004*\"can\" + 0.004*\"face\" + 0.004*\"'re\" + 0.004*\"new\" + 0.004*\"instagram\" + 0.004*\"photo\" + 0.004*\"artist\" + 0.004*\"tried\" + 0.003*\"way\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Values:\n",
      "0\n",
      "\n",
      "***CNTRAVELER***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.021*\"flight\" + 0.018*\"$\" + 0.016*\"deal\" + 0.016*\"world\" + 0.015*\"new\" + 0.011*\"trip\" + 0.008*\"best\" + 0.008*\"u.s.\" + 0.007*\"day\" + 0.007*\"hotel\" + 0.006*\"round-trip\" + 0.006*\"travel\" + 0.006*\"get\" + 0.006*\"airport\" + 0.005*\"fly\" + 0.005*\"cruise\" + 0.005*\"vacation\" + 0.005*\"secret\" + 0.005*\"road\" + 0.005*\"park\" + 0.005*\"...\" + 0.004*\"year\" + 0.004*\"now\" + 0.004*\"paris\" + 0.004*\"next\" + 0.004*\"choice\" + 0.004*\"nyc\" + 0.004*\"one\" + 0.004*\"recipe\" + 0.004*\"change\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.022*\"2016\" + 0.017*\"travel\" + 0.013*\"beautiful\" + 0.012*\"will\" + 0.010*\"photo\" + 0.008*\"make\" + 0.008*\"place\" + 0.008*\"10\" + 0.008*\"world\" + 0.007*\"want\" + 0.007*\"passport\" + 0.006*\"plane\" + 0.006*\"traveler\" + 0.006*\"best\" + 0.005*\"hotel\" + 0.005*\"travelogue\" + 0.005*\"podcast\" + 0.005*\"never\" + 0.005*\"now\" + 0.005*\"new\" + 0.005*\"take\" + 0.005*\"visit\" + 0.005*\"gold\" + 0.005*\"japan\" + 0.004*\"get\" + 0.004*\"wear\" + 0.004*\"train\" + 0.004*\"thing\" + 0.004*\"france\" + 0.004*\"traveling\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.029*\"hotel\" + 0.017*\"new\" + 0.010*\"list\" + 0.009*\"best\" + 0.008*\"like\" + 0.008*\"hot\" + 0.007*\"chef\" + 0.006*\"10\" + 0.006*\"will\" + 0.005*\"global\" + 0.005*\"drink\" + 0.005*\"know\" + 0.005*\"week\" + 0.005*\"beach\" + 0.005*\"stay\" + 0.005*\"inside\" + 0.005*\"room\" + 0.005*\"home\" + 0.004*\"need\" + 0.004*\"cocktail\" + 0.004*\"young\" + 0.004*\"let\" + 0.004*\"pack\" + 0.004*\"see\" + 0.004*\"paris\" + 0.004*\"look\" + 0.004*\"favorite\" + 0.004*\"vacation\" + 0.004*\"travel\" + 0.004*\"editor\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.023*\"new\" + 0.013*\"airline\" + 0.013*\"flight\" + 0.008*\"travel\" + 0.007*\"will\" + 0.007*\"york\" + 0.007*\"london\" + 0.007*\"air\" + 0.006*\"airport\" + 0.006*\"cuba\" + 0.006*\"free\" + 0.006*\"year\" + 0.006*\"fashion\" + 0.006*\"perfect\" + 0.005*\"week\" + 0.005*\"cruise\" + 0.005*\"weekend\" + 0.005*\"first\" + 0.005*\"$\" + 0.005*\"american\" + 0.005*\"passenger\" + 0.005*\"get\" + 0.005*\"plane\" + 0.005*\"jet\" + 0.004*\"delta\" + 0.004*\"way\" + 0.004*\"long\" + 0.004*\"romantic\" + 0.004*\"class\" + 0.003*\"restaurant\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.042*\"best\" + 0.024*\"world\" + 0.017*\"flight\" + 0.014*\"day\" + 0.013*\"city\" + 0.011*\"deal\" + 0.011*\"u.s.\" + 0.010*\"$\" + 0.010*\"now\" + 0.009*\"travel\" + 0.009*\"10\" + 0.007*\"around\" + 0.007*\"right\" + 0.007*\"airline\" + 0.006*\"summer\" + 0.006*\"2017\" + 0.006*\"visit\" + 0.006*\"watch\" + 0.005*\"top\" + 0.005*\"restaurant\" + 0.005*\"go\" + 0.004*\"can\" + 0.004*\"round-trip\" + 0.004*\"u.s\" + 0.004*\"vacation\" + 0.004*\"see\" + 0.004*\"mexico\" + 0.004*\"london\" + 0.004*\"beach\" + 0.004*\"15\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "0\n",
      "\n",
      "***ARCHITECTURALDIGEST***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.057*\"$\" + 0.051*\"million\" + 0.031*\"home\" + 0.010*\"new\" + 0.008*\"selling\" + 0.008*\"guide\" + 0.008*\"california\" + 0.007*\"house\" + 0.007*\"market\" + 0.007*\"former\" + 0.006*\"tour\" + 0.006*\"best\" + 0.006*\"apartment\" + 0.006*\"penthouse\" + 0.005*\"lover\" + 0.005*\"york\" + 0.005*\"11\" + 0.005*\"design\" + 0.005*\"just\" + 0.005*\"spring\" + 0.005*\"hill\" + 0.004*\"estate\" + 0.004*\"travel\" + 0.004*\"thing\" + 0.004*\"10\" + 0.004*\"summer\" + 0.004*\"sale\" + 0.004*\"mansion\" + 0.004*\"hotel\" + 0.003*\"manhattan\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.022*\"home\" + 0.019*\"5\" + 0.018*\"new\" + 0.016*\"design\" + 0.011*\"room\" + 0.011*\"inside\" + 0.010*\"apartment\" + 0.008*\"idea\" + 0.008*\"kitchen\" + 0.007*\"art\" + 0.007*\"york\" + 0.006*\"house\" + 0.006*\"take\" + 0.005*\"beach\" + 0.005*\"tour\" + 0.005*\"bathroom\" + 0.005*\"miami\" + 0.005*\"look\" + 0.005*\"space\" + 0.004*\"exhibition\" + 0.004*\"makeover\" + 0.004*\"7\" + 0.004*\"make\" + 0.004*\"small\" + 0.004*\"best\" + 0.004*\"get\" + 0.004*\"style\" + 0.004*\"manhattan\" + 0.004*\"london\" + 0.004*\"modern\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.029*\"new\" + 0.012*\"york\" + 0.011*\"designer\" + 0.009*\"design\" + 0.008*\"party\" + 0.008*\"7\" + 0.007*\"summer\" + 0.007*\"home\" + 0.006*\"see\" + 0.005*\"6\" + 0.005*\"trip\" + 0.005*\"restaurant\" + 0.005*\"city\" + 0.005*\"favorite\" + 0.005*\"tokyo\" + 0.005*\"architect\" + 0.004*\"architectural\" + 0.004*\"will\" + 0.004*\"bar\" + 0.004*\"interior\" + 0.004*\"art\" + 0.004*\"take\" + 0.003*\"share\" + 0.003*\"unveils\" + 0.003*\"host\" + 0.003*\"get\" + 0.003*\"know\" + 0.003*\"stunning\" + 0.003*\"los\" + 0.003*\"villa\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.014*\"world\" + 0.013*\"new\" + 0.010*\"room\" + 0.009*\"decorate\" + 0.009*\"color\" + 0.007*\"design\" + 0.007*\"way\" + 0.007*\"city\" + 0.007*\"9\" + 0.006*\"stylish\" + 0.006*\"12\" + 0.006*\"space\" + 0.006*\"house\" + 0.006*\"4\" + 0.006*\"hotel\" + 0.006*\"york\" + 0.006*\"will\" + 0.005*\"home\" + 0.005*\"outdoor\" + 0.005*\"architecture\" + 0.005*\"frank\" + 0.005*\"best\" + 0.005*\"10\" + 0.005*\"8\" + 0.005*\"14\" + 0.005*\"idea\" + 0.004*\"building\" + 0.004*\"like\" + 0.004*\"paint\" + 0.004*\"summer\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.016*\"inside\" + 0.015*\"look\" + 0.014*\"home\" + 0.008*\"house\" + 0.007*\"tour\" + 0.006*\"go\" + 0.006*\"hotel\" + 0.005*\"garden\" + 0.005*\"first\" + 0.005*\"renovation\" + 0.005*\"suite\" + 0.005*\"vacation\" + 0.005*\"design\" + 0.005*\"san\" + 0.005*\"museum\" + 0.005*\"art\" + 0.005*\"visit\" + 0.005*\"francisco\" + 0.004*\"will\" + 0.004*\"studio\" + 0.004*\"architecture\" + 0.004*\"fashion\" + 0.004*\"family\" + 0.004*\"designer\" + 0.004*\"building\" + 0.004*\"newest\" + 0.004*\"set\" + 0.004*\"get\" + 0.004*\"contemporary\" + 0.004*\"beautiful\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "0\n",
      "\n",
      "***VOGUE***\n",
      "Initializing:...\n",
      "Generating Model...\n",
      "Topics\n",
      "\n",
      "Topic 0: \n",
      "\n",
      "0.025*\"best\" + 0.019*\"beauty\" + 0.019*\"2016\" + 0.016*\"week\" + 0.016*\"thing\" + 0.016*\"look\" + 0.015*\"know\" + 0.013*\"red\" + 0.012*\"10\" + 0.012*\"carpet\" + 0.011*\"show\" + 0.010*\"fashion\" + 0.009*\"hadid\" + 0.008*\"kendall\" + 0.008*\"5\" + 0.008*\"jenner\" + 0.007*\"fall\" + 0.007*\"golden\" + 0.007*\"gigi\" + 0.006*\"globe\" + 0.006*\"instagrams\" + 0.006*\"hair\" + 0.006*\"oscar\" + 0.005*\"celebrity\" + 0.005*\"jennifer\" + 0.004*\"11\" + 0.004*\"moment\" + 0.004*\"secret\" + 0.004*\"style\" + 0.004*\"take\" \n",
      "\n",
      "Topic 1: \n",
      "\n",
      "0.022*\"new\" + 0.020*\"fashion\" + 0.016*\"2016\" + 0.015*\"model\" + 0.014*\"week\" + 0.014*\"best\" + 0.013*\"style\" + 0.009*\"street\" + 0.009*\"10\" + 0.008*\"meet\" + 0.008*\"men\" + 0.008*\"year\" + 0.007*\"york\" + 0.006*\"show\" + 0.006*\"fall\" + 0.006*\"star\" + 0.005*\"runway\" + 0.005*\"london\" + 0.005*\"need\" + 0.005*\"5\" + 0.005*\"row\" + 0.005*\"girl\" + 0.004*\"bowie\" + 0.004*\"hair\" + 0.004*\"6\" + 0.004*\"front\" + 0.004*\"paris\" + 0.004*\"7\" + 0.004*\"designer\" + 0.004*\"menswear\" \n",
      "\n",
      "Topic 2: \n",
      "\n",
      "0.010*\"globe\" + 0.009*\"golden\" + 0.009*\"met\" + 0.006*\"new\" + 0.006*\"wedding\" + 0.004*\"big\" + 0.004*\"back\" + 0.004*\"show\" + 0.004*\"look\" + 0.004*\"next\" + 0.004*\"blonde\" + 0.003*\"guide\" + 0.003*\"olsen\" + 0.003*\"prada\" + 0.003*\"dress\" + 0.003*\"beautiful\" + 0.003*\"vogue\" + 0.003*\"signature\" + 0.003*\"bad\" + 0.003*\"power\" + 0.003*\"first\" + 0.003*\"take\" + 0.003*\"sunday\" + 0.003*\"man\" + 0.003*\"$\" + 0.003*\"move\" + 0.003*\"see\" + 0.003*\"time\" + 0.003*\"attend\" + 0.003*\"return\" \n",
      "\n",
      "Topic 3: \n",
      "\n",
      "0.029*\"new\" + 0.010*\"look\" + 0.010*\"kate\" + 0.007*\"style\" + 0.006*\"beauty\" + 0.006*\"inside\" + 0.006*\"fashion\" + 0.005*\"take\" + 0.005*\"taylor\" + 0.005*\"york\" + 0.004*\"smith\" + 0.004*\"video\" + 0.004*\"can\" + 0.004*\"back\" + 0.004*\"michael\" + 0.004*\"first\" + 0.004*\"dress\" + 0.003*\"prince\" + 0.003*\"birthday\" + 0.003*\"hair\" + 0.003*\"day\" + 0.003*\"swift\" + 0.003*\"break\" + 0.003*\"5\" + 0.003*\"life\" + 0.003*\"u\" + 0.003*\"home\" + 0.003*\"bag\" + 0.003*\"gala\" + 0.003*\"love\" \n",
      "\n",
      "Topic 4: \n",
      "\n",
      "0.011*\"new\" + 0.008*\"summer\" + 0.008*\"west\" + 0.008*\"vogue\" + 0.008*\"inside\" + 0.007*\"get\" + 0.007*\"style\" + 0.005*\"kardashian\" + 0.005*\"ultimate\" + 0.005*\"day\" + 0.005*\"girl\" + 0.005*\"party\" + 0.005*\"kim\" + 0.005*\"look\" + 0.004*\"beauty\" + 0.004*\"guide\" + 0.004*\"product\" + 0.004*\"winter\" + 0.004*\"campaign\" + 0.004*\"year\" + 0.004*\"like\" + 0.004*\"kanye\" + 0.004*\"one\" + 0.004*\"night\" + 0.003*\"star\" + 0.003*\"house\" + 0.003*\"first\" + 0.003*\"love\" + 0.003*\"lady\" + 0.003*\"go\" \n",
      "\n",
      "Adding topic probabilities to DataFrame...\n",
      "Number of Missing Values:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n***GLAMOUR***\")\n",
    "topic_vector = topic_modeling(glamour,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "# a very few number of posts don't contain all 5 topics, which explains the 'if'statement within. Not worth building\n",
    "# out separate code for so few exceptions\n",
    "df_blogs_glamour['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_glamour['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_glamour['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_glamour['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_glamour['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_glamour.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***TEENVOGUE***\")\n",
    "topic_vector = topic_modeling(teenvogue,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_teenvogue['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_teenvogue['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_teenvogue['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_teenvogue['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_teenvogue['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_teenvogue.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***WMAGAZINE***\")\n",
    "topic_vector = topic_modeling(wmagazine,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_wmagazine['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_wmagazine['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_wmagazine['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_wmagazine['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_wmagazine['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_wmagazine.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***ALLURE***\")\n",
    "topic_vector = topic_modeling(allure,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_allure['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_allure['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_allure['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_allure['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_allure['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_allure.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***CNTRAVELER***\")\n",
    "topic_vector = topic_modeling(cntraveler,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_cntraveler['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_cntraveler['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_cntraveler['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_cntraveler['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_cntraveler['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_cntraveler.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***ARCHITECTURALDIGEST***\")\n",
    "topic_vector = topic_modeling(architecturaldigest,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_architecturaldigest['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_architecturaldigest['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_architecturaldigest['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_architecturaldigest['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_architecturaldigest['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_architecturaldigest.Topic_4.isnull().sum())\n",
    "\n",
    "print(\"\\n***VOGUE***\")\n",
    "topic_vector = topic_modeling(vogue,number_of_passes = 3)\n",
    "print(\"Adding topic probabilities to DataFrame...\")\n",
    "df_blogs_vogue['Topic_0'] = [topic_vector[i][0][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_vogue['Topic_1'] = [topic_vector[i][1][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_vogue['Topic_2'] = [topic_vector[i][2][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_vogue['Topic_3'] = [topic_vector[i][3][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "df_blogs_vogue['Topic_4'] = [topic_vector[i][4][1] if len(topic_vector[i]) == 5 else np.NaN for i in range(len(topic_vector))]\n",
    "print(\"Number of Missing Values:\")\n",
    "print(df_blogs_vogue.Topic_4.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below helps to create a cool vizualiztion\n",
    "# just make sure that you rename all of the models, corpuses, and dictionaries\n",
    "# if you want to be able to see each one\n",
    "# vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "# pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code creates information about the most frequent words\n",
    "# or the most frequent words that stand out from the rest\n",
    "# to add these feature to the dataframe, just merge the dataframes\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tvec20 = CountVectorizer(max_features = 20,stop_words='english')\n",
    "tfidf20  = pd.DataFrame(tvec20.fit_transform(blogs.title).todense(),columns=tvec20.get_feature_names())\n",
    "print(\"Top 20 tf-idf:\")\n",
    "print(tfidf20.sum().sort_values(ascending=False))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tvec40 = CountVectorizer(max_features = 40,stop_words='english')\n",
    "tfidf40  = pd.DataFrame(tvec40.fit_transform(blogs.title).todense(),columns=tvec40.get_feature_names())\n",
    "print(\"Top 40 tf-idf:\")\n",
    "print(tfidf40.sum().sort_values(ascending=False))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "cvec40 = CountVectorizer(max_features = 40,stop_words='english')\n",
    "vec40  = pd.DataFrame(cvec40.fit_transform(blogs.summary).todense(),columns=cvec40.get_feature_names())\n",
    "print(\"Top 40 Most Used Words:\")\n",
    "print(vec40.sum().sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
